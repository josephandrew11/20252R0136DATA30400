{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34537235",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T08:13:28.026073Z",
     "iopub.status.busy": "2025-11-17T08:13:28.025732Z",
     "iopub.status.idle": "2025-11-17T08:13:29.583995Z",
     "shell.execute_reply": "2025-11-17T08:13:29.583444Z",
     "shell.execute_reply.started": "2025-11-17T08:13:28.026040Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "\n",
      "== Data path check ==\n",
      "Amazon_products/train/train_corpus.txt -> True\n",
      "Amazon_products/test/test_corpus.txt -> True\n",
      "Amazon_products/classes.txt -> True\n",
      "Amazon_products/class_hierarchy.txt -> True\n",
      "Amazon_products/class_related_keywords.txt -> True\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# 1. REPRODUCIBILITY SETTINGS \n",
    "# ================================================\n",
    "from pathlib import Path\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "# ----- Reproducibility -----\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# ----- Device -----\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# ================================================\n",
    "# 2. DEFAULT PATHS FOR DATASET\n",
    "# ================================================\n",
    "ROOT = Path(\"Amazon_products\")   # dataset root directory\n",
    "\n",
    "# Main corpus\n",
    "TRAIN_CORPUS_PATH = ROOT / \"train\" /  \"train_corpus.txt\"       # pid \\t text\n",
    "TEST_CORPUS_PATH  = ROOT / \"test\" / \"test_corpus.txt\"        # pid \\t text\n",
    "\n",
    "# Taxonomy & class meta\n",
    "CLASSES_PATH      = ROOT / \"classes.txt\"            # class_id \\t class_name\n",
    "HIERARCHY_PATH    = ROOT / \"class_hierarchy.txt\"    # parent_id \\t child_id\n",
    "KEYWORDS_PATH     = ROOT / \"class_related_keywords.txt\"\n",
    "\n",
    "# Constants\n",
    "NUM_CLASSES = 531\n",
    "MIN_LABELS = 2     # minimum number of labels per sample\n",
    "MAX_LABELS = 3     # maximum number of labels per sample\n",
    "\n",
    "# Check paths\n",
    "print(\"\\n== Data path check ==\")\n",
    "for p in [TRAIN_CORPUS_PATH, TEST_CORPUS_PATH,\n",
    "          CLASSES_PATH, HIERARCHY_PATH, KEYWORDS_PATH]:\n",
    "    print(f\"{p} -> {p.exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95583a7e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T08:13:32.265980Z",
     "iopub.status.busy": "2025-11-17T08:13:32.265628Z",
     "iopub.status.idle": "2025-11-17T08:13:32.404101Z",
     "shell.execute_reply": "2025-11-17T08:13:32.403519Z",
     "shell.execute_reply.started": "2025-11-17T08:13:32.265960Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train/test corpus...\n",
      "Train samples: 29487\n",
      "Test samples : 19658\n",
      "Example train sample #0: pid=0, text=omron hem 790it automatic blood pressure monitor with advanced omron health mana...\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# 2. DATA LOADING\n",
    "# ================================================\n",
    "\n",
    "def load_corpus(path):\n",
    "    \"\"\"\n",
    "    Load corpus file (pid \\\\t text) as {pid: text} dictionary.\n",
    "    \"\"\"\n",
    "    pid2text = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\"\\t\", 1)\n",
    "            if len(parts) == 2:\n",
    "                pid, text = parts\n",
    "                pid2text[pid] = text\n",
    "    return pid2text\n",
    "\n",
    "# Load corpus\n",
    "print(\"Loading train/test corpus...\")\n",
    "\n",
    "pid2text_train = load_corpus(TRAIN_CORPUS_PATH)\n",
    "pid2text_test  = load_corpus(TEST_CORPUS_PATH)\n",
    "\n",
    "pid_list_train = list(pid2text_train.keys())\n",
    "pid_list_test  = list(pid2text_test.keys())\n",
    "\n",
    "print(\"Train samples:\", len(pid2text_train))\n",
    "print(\"Test samples :\", len(pid2text_test))\n",
    "\n",
    "# Quick sample check\n",
    "for i, (pid, text) in enumerate(pid2text_train.items()):\n",
    "    print(f\"Example train sample #{i}: pid={pid}, text={text[:80]}...\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "968916cf-d39c-4884-a813-7daeed03f98e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T08:13:34.696661Z",
     "iopub.status.busy": "2025-11-17T08:13:34.696286Z",
     "iopub.status.idle": "2025-11-17T08:13:34.710186Z",
     "shell.execute_reply": "2025-11-17T08:13:34.709278Z",
     "shell.execute_reply.started": "2025-11-17T08:13:34.696644Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading class metadata...\n",
      "Num classes: 531\n",
      "Num edges in taxonomy: 568\n",
      "\n",
      "Example class id: 0\n",
      "Name: grocery_gourmet_food\n",
      "Keywords: ['snacks', 'condiments', 'beverages', 'specialty_foods', 'spices', 'cooking_oils', 'baking_ingredients', 'gourmet_chocolates', 'artisanal_cheeses', 'organic_foods']\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# 3. CLASS METADATA LOADING\n",
    "# ================================================\n",
    "\n",
    "def load_classes(path):\n",
    "    \"\"\"\n",
    "    classes.txt : class_id \\\\t class_name\n",
    "    returns: id2label, label2id\n",
    "    \"\"\"\n",
    "    id2label = {}\n",
    "    label2id = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\"\\t\")\n",
    "            if len(parts) != 2:\n",
    "                continue\n",
    "            cid, name = parts\n",
    "            cid = int(cid)\n",
    "            id2label[cid] = name\n",
    "            label2id[name] = cid\n",
    "    return id2label, label2id\n",
    "\n",
    "\n",
    "def load_hierarchy(path):\n",
    "    \"\"\"\n",
    "    class_hierarchy.txt : parent_id \\\\t child_id\n",
    "    returns: edges (list of tuples)\n",
    "    \"\"\"\n",
    "    edges = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\"\\t\")\n",
    "            if len(parts) != 2:\n",
    "                continue\n",
    "            p, c = map(int, parts)\n",
    "            edges.append((p, c))\n",
    "    return edges\n",
    "\n",
    "\n",
    "def load_keywords(path, label2id):\n",
    "    \"\"\"\n",
    "    class_related_keywords.txt : CLASS_NAME: kw1, kw2,...\n",
    "    returns: {class_id: [kws]}\n",
    "    \"\"\"\n",
    "    d = {cid: [] for cid in label2id.values()}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if \":\" not in line:\n",
    "                continue\n",
    "            name, kws = line.strip().split(\":\", 1)\n",
    "            kws = [k.strip() for k in kws.split(\",\") if k.strip()]\n",
    "            if name in label2id:\n",
    "                cid = label2id[name]\n",
    "                d[cid] = kws\n",
    "    return d\n",
    "\n",
    "\n",
    "# ----------------- Load all class meta -----------------\n",
    "print(\"Loading class metadata...\")\n",
    "\n",
    "id2label, label2id = load_classes(CLASSES_PATH)\n",
    "edges = load_hierarchy(HIERARCHY_PATH)\n",
    "label_keywords = load_keywords(KEYWORDS_PATH, label2id)\n",
    "\n",
    "print(\"Num classes:\", len(id2label))\n",
    "print(\"Num edges in taxonomy:\", len(edges))\n",
    "print()\n",
    "\n",
    "# Small check\n",
    "example_id = 0\n",
    "print(\"Example class id:\", example_id)\n",
    "print(\"Name:\", id2label[example_id])\n",
    "print(\"Keywords:\", label_keywords[example_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b383df-383e-4b23-949b-1190afd9564e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T08:13:42.060236Z",
     "iopub.status.busy": "2025-11-17T08:13:42.059951Z",
     "iopub.status.idle": "2025-11-17T08:13:52.948104Z",
     "shell.execute_reply": "2025-11-17T08:13:52.947069Z",
     "shell.execute_reply.started": "2025-11-17T08:13:42.060216Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example class name text: grocery gourmet food\n",
      "Example class keyword text: snacks condiments beverages specialty_foods spices cooking_oils baking_ingredients gourmet_chocolates artisanal_cheeses organic_foods\n",
      "\n",
      "Fitting TF-IDF vectorizer on docs + class names + keywords...\n",
      "Vocabulary size: 100000\n",
      "X_train_docs: (29487, 100000)\n",
      "X_test_docs : (19658, 100000)\n",
      "X_class_name: (531, 100000)\n",
      "X_class_kw  : (531, 100000)\n",
      "\n",
      "Normalizing TF-IDF vectors...\n",
      "\n",
      "Computing doc–class cosine similarities...\n",
      "sims_name shape: (29487, 531) | min/max: 0.0 0.6900341\n",
      "sims_kw   shape: (29487, 531) | min/max: 0.0 0.5547385\n",
      "\n",
      "Final sims shape: (29487, 531)\n",
      "Final sims min/max: 0.0 0.6210307\n",
      "\n",
      "Normalized sims:\n",
      "sims_norm shape: (29487, 531)\n",
      "sims_norm min/max: 0.0 1.0\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# 4. TF-IDF EMBEDDING + DOC–CLASS SIMILARITY\n",
    "#    (class name vs keyword 분리 활용)\n",
    "# ================================================\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (1) 클래스 이름 텍스트 / 키워드 텍스트 분리 생성\n",
    "# ------------------------------------------------\n",
    "def build_class_name_texts(id2label):\n",
    "    \"\"\"\n",
    "    각 class_id에 대해 클래스 이름만 사용한 텍스트 리스트 생성\n",
    "    \"\"\"\n",
    "    texts = []\n",
    "    for cid in range(NUM_CLASSES):\n",
    "        name = id2label[cid].replace(\"_\", \" \")\n",
    "        texts.append(name)\n",
    "    return texts\n",
    "\n",
    "def build_class_keyword_texts(label_keywords):\n",
    "    \"\"\"\n",
    "    각 class_id에 대해 키워드만 이어붙인 텍스트 리스트 생성\n",
    "    \"\"\"\n",
    "    texts = []\n",
    "    for cid in range(NUM_CLASSES):\n",
    "        kws = \" \".join(label_keywords.get(cid, []))\n",
    "        texts.append(kws if kws else \"\")\n",
    "    return texts\n",
    "\n",
    "class_name_texts = build_class_name_texts(id2label)         # [C]\n",
    "class_kw_texts   = build_class_keyword_texts(label_keywords) # [C]\n",
    "\n",
    "print(\"Example class name text:\", class_name_texts[0])\n",
    "print(\"Example class keyword text:\", class_kw_texts[0])\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (2) TF-IDF vocabulary 학습\n",
    "#     - train 문서 + test 문서 + class name + class keyword 모두 포함\n",
    "# ------------------------------------------------\n",
    "all_texts_for_vocab = (\n",
    "    list(pid2text_train.values())\n",
    "    + list(pid2text_test.values())\n",
    "    + class_name_texts\n",
    "    + class_kw_texts\n",
    ")\n",
    "\n",
    "print(\"\\nFitting TF-IDF vectorizer on docs + class names + keywords...\")\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=100_000,   # 필요에 따라 조정 가능\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=1                # ★ 중요: class 단어 손실 방지\n",
    ")\n",
    "vectorizer.fit(all_texts_for_vocab)\n",
    "\n",
    "print(\"Vocabulary size:\", len(vectorizer.vocabulary_))\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (3) 실제 TF-IDF 행렬 변환\n",
    "# ------------------------------------------------\n",
    "N_train = len(pid2text_train)\n",
    "N_test  = len(pid2text_test)\n",
    "C       = NUM_CLASSES\n",
    "\n",
    "# 문서 TF-IDF\n",
    "X_train_docs = vectorizer.transform(pid2text_train.values())   # [N_train, V]\n",
    "X_test_docs  = vectorizer.transform(pid2text_test.values())    # [N_test, V]\n",
    "\n",
    "# 클래스 이름 TF-IDF (GNN용 initial feature)\n",
    "X_class_name = vectorizer.transform(class_name_texts)          # [C, V]\n",
    "\n",
    "# 클래스 키워드 TF-IDF (silver label용 보조 sim)\n",
    "X_class_kw   = vectorizer.transform(class_kw_texts)            # [C, V]\n",
    "\n",
    "print(\"X_train_docs:\", X_train_docs.shape)\n",
    "print(\"X_test_docs :\", X_test_docs.shape)\n",
    "print(\"X_class_name:\", X_class_name.shape)\n",
    "print(\"X_class_kw  :\", X_class_kw.shape)\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (4) L2 정규화 (코사인 유사도용)\n",
    "# ------------------------------------------------\n",
    "print(\"\\nNormalizing TF-IDF vectors...\")\n",
    "X_train_norm      = normalize(X_train_docs, axis=1)\n",
    "X_class_name_norm = normalize(X_class_name, axis=1)\n",
    "X_class_kw_norm   = normalize(X_class_kw, axis=1)\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (5) 이름 기반 / 키워드 기반 doc–class 유사도 계산\n",
    "# ------------------------------------------------\n",
    "print(\"\\nComputing doc–class cosine similarities...\")\n",
    "\n",
    "# 이름 기반 similarity\n",
    "sims_name = (X_train_norm @ X_class_name_norm.T).toarray().astype(\"float32\")  # [N_train, C]\n",
    "\n",
    "# 키워드 기반 similarity\n",
    "sims_kw   = (X_train_norm @ X_class_kw_norm.T).toarray().astype(\"float32\")    # [N_train, C]\n",
    "\n",
    "print(\"sims_name shape:\", sims_name.shape, \"| min/max:\", sims_name.min(), sims_name.max())\n",
    "print(\"sims_kw   shape:\", sims_kw.shape,   \"| min/max:\", sims_kw.min(),   sims_kw.max())\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (6) 두 채널을 가중합하여 최종 similarity 생성\n",
    "#     - alpha: class name 비중\n",
    "#     - beta : keyword 비중\n",
    "# ------------------------------------------------\n",
    "alpha = 0.9  # class name 중요도\n",
    "beta  = 0.1  # keyword 중요도\n",
    "\n",
    "sims = alpha * sims_name + beta * sims_kw   # [N_train, C]\n",
    "\n",
    "print(\"\\nFinal sims shape:\", sims.shape)\n",
    "print(\"Final sims min/max:\", sims.min(), sims.max())\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (7) Document-wise Normalization \n",
    "# ------------------------------------------------\n",
    "\n",
    "def zscore_norm(sims):\n",
    "    \"\"\"\n",
    "    Document-wise z-score normalization.\n",
    "    sims: numpy array [N, C]\n",
    "    \"\"\"\n",
    "    sims_mean = sims.mean(axis=1, keepdims=True)\n",
    "    sims_std  = sims.std(axis=1, keepdims=True) + 1e-8\n",
    "    return (sims - sims_mean) / sims_std\n",
    "\n",
    "def minmax_norm(sims):\n",
    "    \"\"\"\n",
    "    Document-wise min-max normalization.\n",
    "    sims: numpy array [N, C]\n",
    "    \"\"\"\n",
    "    sims_min = sims.min(axis=1, keepdims=True)\n",
    "    sims_max = sims.max(axis=1, keepdims=True)\n",
    "    denom = (sims_max - sims_min) + 1e-8\n",
    "    return (sims - sims_min) / denom\n",
    "\n",
    "# ---- Step A: z-score normalization\n",
    "sims_z = zscore_norm(sims)\n",
    "\n",
    "# ---- Step B: positive clipping\n",
    "sims_z = np.maximum(sims_z, 0)\n",
    "\n",
    "# ---- Step C: Min-Max normalization (0~1)\n",
    "sims_norm = minmax_norm(sims_z)\n",
    "\n",
    "print(\"\\nNormalized sims:\")\n",
    "print(\"sims_norm shape:\", sims_norm.shape)\n",
    "print(\"sims_norm min/max:\", sims_norm.min(), sims_norm.max())\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330b238d-d0cb-4e94-a9d0-4e4eab9ed0fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T08:21:05.162081Z",
     "iopub.status.busy": "2025-11-17T08:21:05.161783Z",
     "iopub.status.idle": "2025-11-17T08:21:05.167300Z",
     "shell.execute_reply": "2025-11-17T08:21:05.166523Z",
     "shell.execute_reply.started": "2025-11-17T08:21:05.162060Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Top-K candidate selection module loaded.\n",
      "Example (first document): {137, 179, 87}\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# 5. SIMPLE TOP-K CANDIDATE SELECTION \n",
    "# ================================================\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# ------------------------------------------------\n",
    "# sims_norm 기반 top-k 후보 선택\n",
    "# ------------------------------------------------\n",
    "def select_topk_candidates(sims_norm, min_k=2, max_k=3):\n",
    "    \"\"\"\n",
    "    sims_norm: [C] shape similarity row for one document\n",
    "    return: set of candidate class IDs\n",
    "    \"\"\"\n",
    "    # 1) max_k개를 선택 (가장 높은 similarity부터)\n",
    "    top_k_idx = sims_norm.argsort()[-max_k:][::-1]   # 내림차순\n",
    "\n",
    "    # 2) min_k 보장\n",
    "    if len(top_k_idx) < min_k:\n",
    "        # sims_norm이 이상하게 모두 0일 때 등\n",
    "        # 상위 min_k개를 추가로 선택\n",
    "        top_k_idx = sims_norm.argsort()[-min_k:][::-1]\n",
    "\n",
    "    # set으로 반환\n",
    "    return set(top_k_idx)\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 테스트 \n",
    "# ------------------------------------------------\n",
    "print(\"Simple Top-K candidate selection module loaded.\")\n",
    "print(\"Example (first document):\", select_topk_candidates(sims_norm[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3fc7264-de9d-43df-b757-348d8e5d4fd5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T08:27:39.074558Z",
     "iopub.status.busy": "2025-11-17T08:27:39.074132Z",
     "iopub.status.idle": "2025-11-17T08:27:39.426847Z",
     "shell.execute_reply": "2025-11-17T08:27:39.426304Z",
     "shell.execute_reply.started": "2025-11-17T08:27:39.074537Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating silver labels using simple Top-k scheme...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building silver labels (Top-k): 100%|██████████| 29487/29487 [00:00<00:00, 87613.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "silver_labels_v1 shape: (29487, 531)\n",
      "avg positives per doc: 0.005649717\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# 6. SIMPLE SILVER LABEL GENERATION (Top-k 방식)\n",
    "# ================================================\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def build_silver_labels_topk(sims_norm, min_k=2, max_k=3):\n",
    "    \"\"\"\n",
    "    sims_norm : (N_train, NUM_CLASSES) normalized similarity matrix\n",
    "    return:\n",
    "        silver_y : (N, C) float32 (0/1 labels)\n",
    "    \"\"\"\n",
    "    N, C = sims_norm.shape\n",
    "    silver_y = np.zeros((N, C), dtype=np.float32)\n",
    "\n",
    "    for i in tqdm(range(N), desc=\"Building silver labels (Top-k)\"):\n",
    "\n",
    "        # --- A. 안전하게 row 복사 ---\n",
    "        row = np.array(sims_norm[i], dtype=np.float32).copy()\n",
    "\n",
    "        # --- B. shape 검사 (가장 중요한 체크) ---\n",
    "        assert row.shape[0] == C, \\\n",
    "            f\"Error: sims_norm row shape {row.shape}, expected ({C},). \" \\\n",
    "             \"Check sims_norm axis order.\"\n",
    "\n",
    "        # --- C. row 값이 모두 0이면 Fallback ---\n",
    "        if row.max() == 0:\n",
    "            # similarity 정보가 전혀 없다면 top N classes를 fallback\n",
    "            topk_idx = np.arange(C)[-max_k:]\n",
    "        else:\n",
    "            # --- D. 정상 top-k 선택 ---\n",
    "            # ascending → 마지막 max_k → descending\n",
    "            topk_idx = row.argsort()[-max_k:][::-1]\n",
    "\n",
    "        # --- E. min_k 보장 ---\n",
    "        if len(topk_idx) < min_k:\n",
    "            topk_idx = row.argsort()[-min_k:][::-1]\n",
    "\n",
    "        # --- F. silver label 기록 ---\n",
    "        silver_y[i, topk_idx] = 1.0\n",
    "\n",
    "    return silver_y\n",
    "\n",
    "\n",
    "print(\"Generating silver labels using simple Top-k scheme...\")\n",
    "\n",
    "silver_labels_v1 = build_silver_labels_topk(\n",
    "    sims_norm,\n",
    "    min_k=2,\n",
    "    max_k=3\n",
    ")\n",
    "\n",
    "print(\"silver_labels_v1 shape:\", silver_labels_v1.shape)\n",
    "print(\"avg positives per doc:\", silver_labels_v1.mean(axis=1).mean())\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569ac2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# 7. LABEL-GCN + DOCUMENT-CLASS CLASSIFIER\n",
    "# ================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (1) Build normalized adjacency A_hat for GCN\n",
    "# ------------------------------------------------\n",
    "def build_normalized_adj(num_classes, edges):\n",
    "    \"\"\"\n",
    "    edges: [(parent, child), ...]\n",
    "    출력: A_hat (torch.FloatTensor, [C,C])\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    A = np.zeros((num_classes, num_classes), dtype=np.float32)\n",
    "\n",
    "    # parent-child 연결을 양방향으로 넣기\n",
    "    for p, c in edges:\n",
    "        A[p, c] = 1.0\n",
    "        A[c, p] = 1.0\n",
    "\n",
    "    # self-loop\n",
    "    np.fill_diagonal(A, 1.0)\n",
    "\n",
    "    # D^{-1/2} * A * D^{-1/2}\n",
    "    deg = A.sum(axis=1)\n",
    "    deg_inv_sqrt = np.power(deg, -0.5)\n",
    "    deg_inv_sqrt[np.isinf(deg_inv_sqrt)] = 0.0\n",
    "    D_inv_sqrt = np.diag(deg_inv_sqrt)\n",
    "\n",
    "    A_hat = D_inv_sqrt @ A @ D_inv_sqrt\n",
    "    return torch.from_numpy(A_hat).float()\n",
    "\n",
    "\n",
    "A_hat = build_normalized_adj(NUM_CLASSES, edges).to(device)\n",
    "print(\"A_hat built:\", A_hat.shape)\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (2) Label Encoder: GCN\n",
    "# ------------------------------------------------\n",
    "class LabelGCN(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim=256, num_layers=2, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        dims = [in_dim] + [hidden_dim] * num_layers\n",
    "        self.linears = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            self.linears.append(nn.Linear(dims[i], dims[i+1]))\n",
    "\n",
    "    def forward(self, A_hat, H):\n",
    "        x = H  # [C, in_dim]\n",
    "        for i, lin in enumerate(self.linears):\n",
    "            x = A_hat @ x          # GCN aggregation\n",
    "            x = lin(x)\n",
    "            if i < self.num_layers - 1:\n",
    "                x = F.relu(x)\n",
    "                x = self.dropout(x)\n",
    "        return x  # [C, hidden_dim]\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (3) 전체 classifier: TF-IDF doc → projection → dot with label GNN\n",
    "# ------------------------------------------------\n",
    "class TaxonomyClassifier(nn.Module):\n",
    "    def __init__(self, vocab_dim, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        # 문서 임베딩 projection matrix: V → d\n",
    "        self.doc_proj = nn.Linear(vocab_dim, hidden_dim, bias=False)\n",
    "        \n",
    "        # 라벨 GCN\n",
    "        self.label_gcn = LabelGCN(\n",
    "            in_dim=vocab_dim,     # label initial features = TF-IDF class-name vector\n",
    "            hidden_dim=hidden_dim,\n",
    "            num_layers=2,\n",
    "            dropout=0.5\n",
    "        )\n",
    "\n",
    "    def forward(self, doc_feats, label_feats, A_hat):\n",
    "        \"\"\"\n",
    "        doc_feats: [N, V]   TF-IDF 문서 벡터\n",
    "        label_feats: [C, V] TF-IDF 클래스 (name) 벡터\n",
    "        A_hat: [C, C]       taxonomy\n",
    "        \"\"\"\n",
    "        # 1) Document embedding\n",
    "        doc_emb = self.doc_proj(doc_feats)     # [N, d]\n",
    "\n",
    "        # 2) Label embedding via GCN\n",
    "        label_emb = self.label_gcn(A_hat, label_feats)  # [C, d]\n",
    "\n",
    "        # 3) Matching score \n",
    "        logits = doc_emb @ label_emb.T         # [N, C]\n",
    "\n",
    "        return logits, doc_emb, label_emb\n",
    "\n",
    "\n",
    "print(\"Model definitions loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d716f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# 8. ROUND 1 TRAINING WITH SILVER LABELS v1\n",
    "# ================================================\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (1) Dataset 정의 (sparse TF-IDF → batch마다 dense 변환)\n",
    "# ------------------------------------------------\n",
    "class SilverDataset(Dataset):\n",
    "    def __init__(self, X_csr, y_np, pid_list=None):\n",
    "        \"\"\"\n",
    "        X_csr : scipy.sparse CSR matrix (N, V)\n",
    "        y_np  : numpy array (N, C)  -> silver_labels_v1\n",
    "        pid_list : (옵션) pid 리스트, 나중에 쓸 수도 있음\n",
    "        \"\"\"\n",
    "        self.X = X_csr\n",
    "        self.y = y_np\n",
    "        self.pid_list = pid_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 1) sparse → dense (1, V) → (V,)\n",
    "        x_dense = self.X[idx].toarray().astype(\"float32\").squeeze(0)\n",
    "        y = self.y[idx].astype(\"float32\")\n",
    "        return x_dense, y\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (2) Dataset / DataLoader 생성\n",
    "# ------------------------------------------------\n",
    "train_dataset = SilverDataset(X_train_docs, silver_labels_v1, pid_list_train)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,      \n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(\"Train dataset size:\", len(train_dataset))\n",
    "print(\"Batch size:\", batch_size)\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (3) 모델 / optimizer / loss 정의\n",
    "# ------------------------------------------------\n",
    "vocab_dim = X_train_docs.shape[1]\n",
    "hidden_dim = 256\n",
    "\n",
    "model = TaxonomyClassifier(vocab_dim=vocab_dim, hidden_dim=hidden_dim).to(device)\n",
    "\n",
    "# Label initial features (GCN 입력용) : class name TF-IDF만 사용\n",
    "label_feats = torch.from_numpy(\n",
    "    X_class_name.toarray().astype(\"float32\")\n",
    ").to(device)   # [C, V]\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (4) Training loop (Round 1)\n",
    "# ------------------------------------------------\n",
    "num_epochs = 2  # 일단 가볍게 1~2 epoch 정도부터 시도\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch_x, batch_y in tqdm(train_loader, desc=f\"Epoch {epoch}\"):\n",
    "        batch_x = batch_x.to(device)        # [B, V]\n",
    "        batch_y = batch_y.to(device)        # [B, C]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits, doc_emb, label_emb = model(batch_x, label_feats, A_hat)  # [B, C]\n",
    "        loss = criterion(logits, batch_y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * batch_x.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(train_dataset)\n",
    "    print(f\"[Epoch {epoch}] avg_loss = {avg_loss:.4f}\")\n",
    "\n",
    "print(\"Round 1 training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655c7443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# 9. SELF-TRAINING: BUILD SILVER LABELS v2 FROM ROUND 1 MODEL\n",
    "# ================================================\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (1) Round 1 prediction 함수 (train 전체 predict)\n",
    "# ------------------------------------------------\n",
    "def predict_train_probs(model, X_csr, label_feats, A_hat, batch_size=64):\n",
    "    \"\"\"\n",
    "    Train 전체 문서에 대해 확률 예측을 반환한다.\n",
    "    return: probs (N_train, NUM_CLASSES)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    N = X_csr.shape[0]\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, N, batch_size), desc=\"Predicting Round 1\"):\n",
    "            X_batch = X_csr[i : i+batch_size].toarray().astype(\"float32\")\n",
    "            X_batch = torch.from_numpy(X_batch).to(device)\n",
    "\n",
    "            logits, _, _ = model(X_batch, label_feats, A_hat)\n",
    "            probs = torch.sigmoid(logits).cpu().numpy()\n",
    "\n",
    "            all_probs.append(probs)\n",
    "\n",
    "    return np.vstack(all_probs)\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (2) Round 1 전체 예측 수행\n",
    "# ------------------------------------------------\n",
    "print(\"Predicting train probabilities (Round 1)...\")\n",
    "\n",
    "probs_train = predict_train_probs(\n",
    "    model,\n",
    "    X_train_docs,\n",
    "    label_feats,\n",
    "    A_hat,\n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "print(\"probs_train shape:\", probs_train.shape)  # (N_train, 531)\n",
    "print(\"probs range:\", probs_train.min(), probs_train.max())\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (3) Silver labels v2 생성\n",
    "#     adaptive threshold:\n",
    "#       HIGH = 0.7  → confident positive\n",
    "#       LOW  = 0.3  → confident negative\n",
    "# ------------------------------------------------\n",
    "def build_silver_labels_v2(probs, silver_v1, high=0.7, low=0.3):\n",
    "    \"\"\"\n",
    "    probs : round 1 predicted probabilities (N, C)\n",
    "    silver_v1 : previous silver labels (N, C)\n",
    "    return: updated silver_v2 (N, C)\n",
    "    \"\"\"\n",
    "    silver_v2 = silver_v1.copy()\n",
    "\n",
    "    pos_mask = probs >= high\n",
    "    neg_mask = probs <= low\n",
    "\n",
    "    silver_v2[pos_mask] = 1.0\n",
    "    silver_v2[neg_mask] = 0.0\n",
    "\n",
    "    return silver_v2\n",
    "\n",
    "\n",
    "silver_labels_v2 = build_silver_labels_v2(\n",
    "    probs_train,\n",
    "    silver_labels_v1,\n",
    "    high=0.7,\n",
    "    low=0.3\n",
    ")\n",
    "\n",
    "print(\"silver_labels_v2 shape:\", silver_labels_v2.shape)\n",
    "print(\"avg positives per doc:\", silver_labels_v2.sum(axis=1).mean())\n",
    "print(\"Self-training Round 2 labels ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07bbb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# 10. ROUND 2 TRAINING WITH SILVER LABELS v2\n",
    "# ================================================\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# SilverDataset 클래스는 이미 위에서 정의해 둔 것을 재사용:\n",
    "# class SilverDataset(Dataset):\n",
    "#     def __init__(self, X_csr, y_np, pid_list=None):\n",
    "#         ...\n",
    "#     def __len__(self):\n",
    "#         ...\n",
    "#     def __getitem__(self, idx):\n",
    "#         ...\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (1) Round 2용 Dataset / DataLoader 생성\n",
    "# ------------------------------------------------\n",
    "train_dataset_v2 = SilverDataset(X_train_docs, silver_labels_v2, pid_list_train)\n",
    "\n",
    "batch_size_round2 = 64\n",
    "train_loader_v2 = DataLoader(\n",
    "    train_dataset_v2,\n",
    "    batch_size=batch_size_round2,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(\"Round 2 train dataset size:\", len(train_dataset_v2))\n",
    "print(\"Round 2 batch size:\", batch_size_round2)\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (2) Optimizer / Loss 재설정 (모델은 Round 1에서 이어서 사용)\n",
    "# ------------------------------------------------\n",
    "criterion_round2 = nn.BCEWithLogitsLoss()\n",
    "optimizer_round2 = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (3) Round 2 Training Loop\n",
    "# ------------------------------------------------\n",
    "num_epochs_round2 = 2  # Round 2에서도 1~2 epoch 정도 돌려보는 것을 추천\n",
    "\n",
    "for epoch in range(1, num_epochs_round2 + 1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch_x, batch_y in tqdm(train_loader_v2, desc=f\"[Round 2] Epoch {epoch}\"):\n",
    "        batch_x = batch_x.to(device)   # [B, V]\n",
    "        batch_y = batch_y.to(device)   # [B, C]\n",
    "\n",
    "        optimizer_round2.zero_grad()\n",
    "\n",
    "        # label_feats, A_hat, model 은 Round 1에서 이미 정의된 것을 사용\n",
    "        logits, doc_emb, label_emb = model(batch_x, label_feats, A_hat)  # [B, C]\n",
    "        loss = criterion_round2(logits, batch_y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer_round2.step()\n",
    "\n",
    "        total_loss += loss.item() * batch_x.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(train_dataset_v2)\n",
    "    print(f\"[Round 2 - Epoch {epoch}] avg_loss = {avg_loss:.4f}\")\n",
    "\n",
    "print(\"Round 2 training finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806fe56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# 11. TEST PREDICTION + SUBMISSION FILE\n",
    "# ================================================\n",
    "\n",
    "def predict_test_probs(model, X_csr, label_feats, A_hat, batch_size=64):\n",
    "    \"\"\"\n",
    "    Test 전체 문서에 대해 확률 예측을 반환한다.\n",
    "    return: probs (N_test, NUM_CLASSES)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    N = X_csr.shape[0]\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, N, batch_size), desc=\"Predicting Test Set\"):\n",
    "            X_batch = X_csr[i : i+batch_size].toarray().astype(\"float32\")\n",
    "            X_batch = torch.from_numpy(X_batch).to(device)\n",
    "\n",
    "            logits, _, _ = model(X_batch, label_feats, A_hat)\n",
    "            probs = torch.sigmoid(logits).cpu().numpy()\n",
    "\n",
    "            all_probs.append(probs)\n",
    "\n",
    "    return np.vstack(all_probs)\n",
    "\n",
    "\n",
    "print(\"Predicting on test corpus...\")\n",
    "probs_test = predict_test_probs(\n",
    "    model,\n",
    "    X_test_docs,\n",
    "    label_feats,\n",
    "    A_hat,\n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "print(\"probs_test shape:\", probs_test.shape)  # (N_test, 531)\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (2) 문서당 Top-K label 선택\n",
    "# ------------------------------------------------\n",
    "\n",
    "MIN_LABELS = 2\n",
    "MAX_LABELS = 3\n",
    "\n",
    "def pick_labels(prob_row, min_k=1, max_k=3):\n",
    "    \"\"\"\n",
    "    한 문서의 확률벡터에서 top-K 라벨 선택.\n",
    "    \"\"\"\n",
    "    # 확률이 높은 class 순으로 정렬\n",
    "    sorted_idx = np.argsort(prob_row)[::-1]  # 내림차순\n",
    "    topk = sorted_idx[:max_k]\n",
    "    return list(topk)\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (3) submission.csv 파일 생성\n",
    "# ------------------------------------------------\n",
    "print(\"Generating submission.csv...\")\n",
    "\n",
    "with open(SUBMISSION_PATH, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"pid\", \"labels\"])  # header\n",
    "\n",
    "    for i, pid in enumerate(pid_list_test):\n",
    "        labels = pick_labels(probs_test[i], MIN_LABELS, MAX_LABELS)\n",
    "        label_str = \",\".join(map(str, labels))\n",
    "        writer.writerow([pid, label_str])\n",
    "\n",
    "print(\"Submission file saved to:\", SUBMISSION_PATH)\n",
    "print(\"Total test samples:\", len(pid_list_test))\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ce8f55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063eab09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c062713b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886663cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084431fe-97fd-442e-9bb7-78f1153f80c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b486e5-1fb7-4e9e-b63a-bc3919ab81ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e97ef5e-5ee1-4bb1-9ccd-d0242241c2b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312fa965-ac9f-4255-a3d7-3466aba485f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7419c2a8-6739-4ed6-b9ee-3eeb108873de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating dummy predictions: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 19658/19658 [00:00<00:00, 190266.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy submission file saved to: submission.csv\n",
      "Total samples: 19658, Classes per sample: 1-3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ------------------------\n",
    "# Dummy baseline for Kaggle submission\n",
    "# Generates random multi-label predictions\n",
    "# ------------------------\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Paths ---\n",
    "TEST_DIR = \"Amazon_products/test\"  # modify if needed\n",
    "TEST_CORPUS_PATH = os.path.join(TEST_DIR, \"test_corpus.txt\")  # product_id \\t text\n",
    "SUBMISSION_PATH = \"submission.csv\"  # output file\n",
    "\n",
    "# --- Constants ---\n",
    "NUM_CLASSES = 531  # total number of classes (0–530)\n",
    "MIN_LABELS = 1     # minimum number of labels per sample\n",
    "MAX_LABELS = 3     # maximum number of labels per sample\n",
    "\n",
    "# --- Load test corpus ---\n",
    "def load_corpus(path):\n",
    "    \"\"\"Load test corpus into {pid: text} dictionary.\"\"\"\n",
    "    pid2text = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\"\\t\", 1)\n",
    "            if len(parts) == 2:\n",
    "                pid, text = parts\n",
    "                pid2text[pid] = text\n",
    "    return pid2text\n",
    "\n",
    "pid2text_test = load_corpus(TEST_CORPUS_PATH)\n",
    "pid_list_test = list(pid2text_test.keys())\n",
    "\n",
    "# --- Generate random predictions ---\n",
    "all_pids, all_labels = [], []\n",
    "for pid in tqdm(pid_list_test, desc=\"Generating dummy predictions\"):\n",
    "    n_labels = random.randint(MIN_LABELS, MAX_LABELS)\n",
    "    labels = random.sample(range(NUM_CLASSES), n_labels)\n",
    "    labels = sorted(labels)\n",
    "    all_pids.append(pid)\n",
    "    all_labels.append(labels)\n",
    "\n",
    "# --- Save submission file ---\n",
    "with open(SUBMISSION_PATH, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"pid\", \"labels\"])\n",
    "    for pid, labels in zip(all_pids, all_labels):\n",
    "        writer.writerow([pid, \",\".join(map(str, labels))])\n",
    "\n",
    "print(f\"Dummy submission file saved to: {SUBMISSION_PATH}\")\n",
    "print(f\"Total samples: {len(all_pids)}, Classes per sample: {MIN_LABELS}-{MAX_LABELS}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ESCI Environment (Python 3.9)",
   "language": "python",
   "name": "esci"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
