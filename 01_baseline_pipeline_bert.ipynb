{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34537235",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T12:15:30.966573Z",
     "iopub.status.busy": "2025-11-30T12:15:30.966317Z",
     "iopub.status.idle": "2025-11-30T12:15:32.490451Z",
     "shell.execute_reply": "2025-11-30T12:15:32.489764Z",
     "shell.execute_reply.started": "2025-11-30T12:15:30.966545Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "\n",
      "== Data path check ==\n",
      "Amazon_products/train/train_corpus.txt -> True\n",
      "Amazon_products/test/test_corpus.txt -> True\n",
      "Amazon_products/classes.txt -> True\n",
      "Amazon_products/class_hierarchy.txt -> True\n",
      "Amazon_products/class_related_keywords.txt -> True\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# 1. REPRODUCIBILITY SETTINGS \n",
    "# ================================================\n",
    "from pathlib import Path\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "# ----- Reproducibility -----\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# ----- Device -----\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# ================================================\n",
    "# 2. DEFAULT PATHS FOR DATASET\n",
    "# ================================================\n",
    "ROOT = Path(\"Amazon_products\")   # dataset root directory\n",
    "\n",
    "# Main corpus\n",
    "TRAIN_CORPUS_PATH = ROOT / \"train\" /  \"train_corpus.txt\"       # pid \\t text\n",
    "TEST_CORPUS_PATH  = ROOT / \"test\" / \"test_corpus.txt\"        # pid \\t text\n",
    "\n",
    "# Taxonomy & class meta\n",
    "CLASSES_PATH      = ROOT / \"classes.txt\"            # class_id \\t class_name\n",
    "HIERARCHY_PATH    = ROOT / \"class_hierarchy.txt\"    # parent_id \\t child_id\n",
    "KEYWORDS_PATH     = ROOT / \"class_related_keywords.txt\"\n",
    "\n",
    "# Constants\n",
    "NUM_CLASSES = 531\n",
    "MIN_LABELS = 2     # minimum number of labels per sample\n",
    "MAX_LABELS = 3     # maximum number of labels per sample\n",
    "\n",
    "# Check paths\n",
    "print(\"\\n== Data path check ==\")\n",
    "for p in [TRAIN_CORPUS_PATH, TEST_CORPUS_PATH,\n",
    "          CLASSES_PATH, HIERARCHY_PATH, KEYWORDS_PATH]:\n",
    "    print(f\"{p} -> {p.exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95583a7e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T12:15:32.491518Z",
     "iopub.status.busy": "2025-11-30T12:15:32.491270Z",
     "iopub.status.idle": "2025-11-30T12:15:32.636374Z",
     "shell.execute_reply": "2025-11-30T12:15:32.635363Z",
     "shell.execute_reply.started": "2025-11-30T12:15:32.491495Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train/test corpus...\n",
      "Train samples: 29487\n",
      "Test samples : 19658\n",
      "Example train sample #0: pid=0, text=omron hem 790it automatic blood pressure monitor with advanced omron health mana...\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# 2. DATA LOADING\n",
    "# ================================================\n",
    "\n",
    "def load_corpus(path):\n",
    "    \"\"\"\n",
    "    Load corpus file (pid \\\\t text) as {pid: text} dictionary.\n",
    "    \"\"\"\n",
    "    pid2text = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\"\\t\", 1)\n",
    "            if len(parts) == 2:\n",
    "                pid, text = parts\n",
    "                pid2text[pid] = text\n",
    "    return pid2text\n",
    "\n",
    "# Load corpus\n",
    "print(\"Loading train/test corpus...\")\n",
    "\n",
    "pid2text_train = load_corpus(TRAIN_CORPUS_PATH)\n",
    "pid2text_test  = load_corpus(TEST_CORPUS_PATH)\n",
    "\n",
    "pid_list_train = list(pid2text_train.keys())\n",
    "pid_list_test  = list(pid2text_test.keys())\n",
    "\n",
    "print(\"Train samples:\", len(pid2text_train))\n",
    "print(\"Test samples :\", len(pid2text_test))\n",
    "\n",
    "# Quick sample check\n",
    "for i, (pid, text) in enumerate(pid2text_train.items()):\n",
    "    print(f\"Example train sample #{i}: pid={pid}, text={text[:80]}...\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "968916cf-d39c-4884-a813-7daeed03f98e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T12:15:35.013278Z",
     "iopub.status.busy": "2025-11-30T12:15:35.012997Z",
     "iopub.status.idle": "2025-11-30T12:15:35.026436Z",
     "shell.execute_reply": "2025-11-30T12:15:35.025515Z",
     "shell.execute_reply.started": "2025-11-30T12:15:35.013256Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading class metadata...\n",
      "Num classes: 531\n",
      "Num edges in taxonomy: 568\n",
      "\n",
      "Example class id: 0\n",
      "Name: grocery_gourmet_food\n",
      "Keywords: ['snacks', 'condiments', 'beverages', 'specialty_foods', 'spices', 'cooking_oils', 'baking_ingredients', 'gourmet_chocolates', 'artisanal_cheeses', 'organic_foods']\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# 3. CLASS METADATA LOADING\n",
    "# ================================================\n",
    "\n",
    "def load_classes(path):\n",
    "    \"\"\"\n",
    "    classes.txt : class_id \\\\t class_name\n",
    "    returns: id2label, label2id\n",
    "    \"\"\"\n",
    "    id2label = {}\n",
    "    label2id = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\"\\t\")\n",
    "            if len(parts) != 2:\n",
    "                continue\n",
    "            cid, name = parts\n",
    "            cid = int(cid)\n",
    "            id2label[cid] = name\n",
    "            label2id[name] = cid\n",
    "    return id2label, label2id\n",
    "\n",
    "\n",
    "def load_hierarchy(path):\n",
    "    \"\"\"\n",
    "    class_hierarchy.txt : parent_id \\\\t child_id\n",
    "    returns: edges (list of tuples)\n",
    "    \"\"\"\n",
    "    edges = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\"\\t\")\n",
    "            if len(parts) != 2:\n",
    "                continue\n",
    "            p, c = map(int, parts)\n",
    "            edges.append((p, c))\n",
    "    return edges\n",
    "\n",
    "\n",
    "def load_keywords(path, label2id):\n",
    "    \"\"\"\n",
    "    class_related_keywords.txt : CLASS_NAME: kw1, kw2,...\n",
    "    returns: {class_id: [kws]}\n",
    "    \"\"\"\n",
    "    d = {cid: [] for cid in label2id.values()}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if \":\" not in line:\n",
    "                continue\n",
    "            name, kws = line.strip().split(\":\", 1)\n",
    "            kws = [k.strip() for k in kws.split(\",\") if k.strip()]\n",
    "            if name in label2id:\n",
    "                cid = label2id[name]\n",
    "                d[cid] = kws\n",
    "    return d\n",
    "\n",
    "\n",
    "# ----------------- Load all class meta -----------------\n",
    "print(\"Loading class metadata...\")\n",
    "\n",
    "id2label, label2id = load_classes(CLASSES_PATH)\n",
    "edges = load_hierarchy(HIERARCHY_PATH)\n",
    "label_keywords = load_keywords(KEYWORDS_PATH, label2id)\n",
    "\n",
    "print(\"Num classes:\", len(id2label))\n",
    "print(\"Num edges in taxonomy:\", len(edges))\n",
    "print()\n",
    "\n",
    "# Small check\n",
    "example_id = 0\n",
    "print(\"Example class id:\", example_id)\n",
    "print(\"Name:\", id2label[example_id])\n",
    "print(\"Keywords:\", label_keywords[example_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19b383df-383e-4b23-949b-1190afd9564e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T12:15:36.409790Z",
     "iopub.status.busy": "2025-11-30T12:15:36.409552Z",
     "iopub.status.idle": "2025-11-30T12:18:09.195487Z",
     "shell.execute_reply": "2025-11-30T12:18:09.194892Z",
     "shell.execute_reply.started": "2025-11-30T12:15:36.409772Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/esci/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded BERT model: bert-base-uncased | device: cuda\n",
      "Encoding train documents with BERT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BERT encoding train docs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 922/922 [01:27<00:00, 10.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding test documents with BERT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BERT encoding test docs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 615/615 [01:00<00:00, 10.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_emb_train: torch.Size([29487, 768])\n",
      "doc_emb_test : torch.Size([19658, 768])\n",
      "Example class text 0: grocery gourmet food. Related keywords: snacks condiments beverages specialty_foods spices cooking_oils baking_ingredients gourmet_chocolates artisanal_cheeses organic_foods\n",
      "Encoding class texts with BERT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BERT encoding class texts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00<00:00, 39.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_emb: torch.Size([531, 768])\n",
      "\n",
      "Computing BERT-based docâ€“class cosine similarities...\n",
      "sims shape: (29487, 531)\n",
      "sims min/max: 0.23111407 0.8482967\n",
      "Done (BERT sims for train).\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# 4. BERT EMBEDDING + DOCâ€“CLASS SIMILARITY\n",
    "# ================================================\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (1) BERT ëª¨ë¸ ì¤€ë¹„\n",
    "# ------------------------------------------------\n",
    "MODEL_NAME = \"bert-base-uncased\"  # í•„ìš”í•˜ë©´ ë‹¤ë¥¸ ëª¨ë¸ë¡œ êµì²´ ê°€ëŠ¥ (ex. roberta-base ë“±)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "bert_model = AutoModel.from_pretrained(MODEL_NAME).to(device)\n",
    "bert_model.eval()\n",
    "\n",
    "print(\"Loaded BERT model:\", MODEL_NAME, \"| device:\", device)\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (2) mean pooling í•¨ìˆ˜ ì •ì˜\n",
    "# ------------------------------------------------\n",
    "def mean_pooling(last_hidden_state, attention_mask):\n",
    "    \"\"\"\n",
    "    last_hidden_state: [B, L, H]\n",
    "    attention_mask   : [B, L]\n",
    "    \"\"\"\n",
    "    mask = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "    sum_embeddings = (last_hidden_state * mask).sum(dim=1)          # [B, H]\n",
    "    sum_mask = mask.sum(dim=1).clamp(min=1e-9)                      # [B, H]\n",
    "    return sum_embeddings / sum_mask                                # [B, H]\n",
    "\n",
    "@torch.no_grad()\n",
    "def encode_texts_with_bert(texts, batch_size=32, max_length=128, desc=\"Encoding\"):\n",
    "    \"\"\"\n",
    "    texts: list of str\n",
    "    return: torch.FloatTensor [N, H] (CPU í…ì„œ)\n",
    "    \"\"\"\n",
    "    all_embs = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=desc):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        enc = tokenizer(\n",
    "            batch,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "\n",
    "        outputs = bert_model(**enc)\n",
    "        emb = mean_pooling(outputs.last_hidden_state, enc[\"attention_mask\"])  # [B, H]\n",
    "        all_embs.append(emb.cpu())\n",
    "\n",
    "    return torch.cat(all_embs, dim=0)  # [N, H]\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (3) train/test ë¬¸ì„œ ì„ë² ë”©\n",
    "# ------------------------------------------------\n",
    "train_texts = [pid2text_train[pid] for pid in pid_list_train]\n",
    "test_texts  = [pid2text_test[pid]  for pid in pid_list_test]\n",
    "\n",
    "print(\"Encoding train documents with BERT...\")\n",
    "doc_emb_train = encode_texts_with_bert(\n",
    "    train_texts,\n",
    "    batch_size=32,\n",
    "    max_length=128,\n",
    "    desc=\"BERT encoding train docs\"\n",
    ")   # [N_train, H]\n",
    "\n",
    "print(\"Encoding test documents with BERT...\")\n",
    "doc_emb_test = encode_texts_with_bert(\n",
    "    test_texts,\n",
    "    batch_size=32,\n",
    "    max_length=128,\n",
    "    desc=\"BERT encoding test docs\"\n",
    ")    # [N_test, H]\n",
    "\n",
    "print(\"doc_emb_train:\", doc_emb_train.shape)\n",
    "print(\"doc_emb_test :\", doc_emb_test.shape)\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (4) í´ë˜ìŠ¤ í…ìŠ¤íŠ¸ (ì´ë¦„ + í‚¤ì›Œë“œ) ì„ë² ë”©\n",
    "# ------------------------------------------------\n",
    "def build_class_text(cid):\n",
    "    \"\"\"\n",
    "    í´ë˜ìŠ¤ ì´ë¦„ + í‚¤ì›Œë“œë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ë¬¶ì–´ì„œ BERTì— ë„£ì„ í…ìŠ¤íŠ¸ ìƒì„±\n",
    "    \"\"\"\n",
    "    name = id2label[cid].replace(\"_\", \" \")\n",
    "    kws  = \" \".join(label_keywords.get(cid, []))\n",
    "    if kws:\n",
    "        return f\"{name}. Related keywords: {kws}\"\n",
    "    else:\n",
    "        return name\n",
    "\n",
    "class_texts = [build_class_text(cid) for cid in range(NUM_CLASSES)]\n",
    "\n",
    "print(\"Example class text 0:\", class_texts[0])\n",
    "\n",
    "print(\"Encoding class texts with BERT...\")\n",
    "class_emb = encode_texts_with_bert(\n",
    "    class_texts,\n",
    "    batch_size=32,\n",
    "    max_length=32,\n",
    "    desc=\"BERT encoding class texts\"\n",
    ")   # [C, H]\n",
    "\n",
    "print(\"class_emb:\", class_emb.shape)\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (5) ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ docâ€“class similarity ê³„ì‚°\n",
    "# ------------------------------------------------\n",
    "print(\"\\nComputing BERT-based docâ€“class cosine similarities...\")\n",
    "\n",
    "# L2 ì •ê·œí™”\n",
    "doc_emb_train_norm = F.normalize(doc_emb_train, p=2, dim=1)   # [N_train, H]\n",
    "class_emb_norm     = F.normalize(class_emb,     p=2, dim=1)   # [C, H]\n",
    "\n",
    "# sims_train: [N_train, C]\n",
    "sims = (doc_emb_train_norm @ class_emb_norm.T).numpy().astype(\"float32\")\n",
    "\n",
    "print(\"sims shape:\", sims.shape)\n",
    "print(\"sims min/max:\", sims.min(), sims.max())\n",
    "print(\"Done (BERT sims for train).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a7def73-643b-4f4e-a566-6aae3ac0531d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T12:19:21.200362Z",
     "iopub.status.busy": "2025-11-30T12:19:21.200090Z",
     "iopub.status.idle": "2025-11-30T12:19:38.130971Z",
     "shell.execute_reply": "2025-11-30T12:19:38.130362Z",
     "shell.execute_reply.started": "2025-11-30T12:19:21.200342Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Saved] train_doc_emb.jsonl  (29487 lines)\n",
      "[Saved] test_doc_emb.jsonl  (19658 lines)\n",
      "[Saved] class_emb.jsonl  (531 lines)\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# SAVE BERT EMBEDDINGS TO JSONL (one JSON per line)\n",
    "# ================================================\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def save_embeddings_jsonl(pid_list, emb_tensor, save_path):\n",
    "    \"\"\"\n",
    "    pid_list   : list of IDs (ex. product IDs, class IDs)\n",
    "    emb_tensor : torch.Tensor [N, H] (CPU or GPU)\n",
    "    save_path  : output JSONL path\n",
    "    \"\"\"\n",
    "    # numpy ë³€í™˜ (GPU -> CPU ìë™ ì „í™˜)\n",
    "    emb_np = emb_tensor.detach().cpu().numpy()\n",
    "\n",
    "    with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for pid, vec in zip(pid_list, emb_np):\n",
    "            item = {\n",
    "                \"id\": pid,\n",
    "                \"embed\": vec.tolist()      # JSON ì§ë ¬í™”ë¥¼ ìœ„í•´ listë¡œ ë³€í™˜\n",
    "            }\n",
    "            f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "    print(f\"[Saved] {save_path}  ({len(pid_list)} lines)\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# ì‹¤ì œ ì €ì¥ ì‹¤í–‰\n",
    "# ------------------------------------------------\n",
    "\n",
    "# 1) Train ë¬¸ì„œ ì„ë² ë”©\n",
    "save_embeddings_jsonl(\n",
    "    pid_list_train,\n",
    "    doc_emb_train,\n",
    "    \"train_doc_emb.jsonl\"\n",
    ")\n",
    "\n",
    "# 2) Test ë¬¸ì„œ ì„ë² ë”©\n",
    "save_embeddings_jsonl(\n",
    "    pid_list_test,\n",
    "    doc_emb_test,\n",
    "    \"test_doc_emb.jsonl\"\n",
    ")\n",
    "\n",
    "# 3) Class ì„ë² ë”©\n",
    "save_embeddings_jsonl(\n",
    "    list(range(NUM_CLASSES)),   # class IDs = 0..530\n",
    "    class_emb,\n",
    "    \"class_emb.jsonl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "330b238d-d0cb-4e94-a9d0-4e4eab9ed0fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T12:21:02.367720Z",
     "iopub.status.busy": "2025-11-30T12:21:02.367453Z",
     "iopub.status.idle": "2025-11-30T12:21:02.372662Z",
     "shell.execute_reply": "2025-11-30T12:21:02.372143Z",
     "shell.execute_reply.started": "2025-11-30T12:21:02.367702Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Top-K candidate selection (BERT version) loaded.\n",
      "Example (first document): {145, 514, 241}\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# 5. SIMPLE TOP-K CANDIDATE SELECTION  (BERT version)\n",
    "# ================================================\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def select_topk_candidates(sims_row, min_k=2, max_k=3):\n",
    "    \"\"\"\n",
    "    sims_row: shape (C,) - one document's BERT-based cosine similarities\n",
    "               ex) sims[i]  or sims_train[i]\n",
    "    return: set of candidate class IDs\n",
    "    \"\"\"\n",
    "\n",
    "    # sims_row ê°’ì´ ëª¨ë‘ 0 ë˜ëŠ” ìŒìˆ˜ì¼ ìˆ˜ë„ ìˆìœ¼ë¯€ë¡œ np.nan ì œê±°\n",
    "    sims_clean = np.nan_to_num(sims_row, nan=0.0)\n",
    "\n",
    "    # 1) similarity ê¸°ì¤€ ìƒìœ„ max_k ì„ íƒ\n",
    "    top_k_idx = sims_clean.argsort()[-max_k:][::-1]   # ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬\n",
    "\n",
    "    # 2) min_k ë³´ì¥ (BERT ìœ ì‚¬ë„ê°€ ë‚®ì€ ë¬¸ì„œì—ì„œë„ ìµœì†Œ í›„ë³´ ë³´ì¥)\n",
    "    if len(top_k_idx) < min_k:\n",
    "        top_k_idx = sims_clean.argsort()[-min_k:][::-1]\n",
    "\n",
    "    # set í˜•íƒœë¡œ ë°˜í™˜\n",
    "    return set(map(int, top_k_idx))\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Test (example)\n",
    "# ------------------------------------------------\n",
    "print(\"Simple Top-K candidate selection (BERT version) loaded.\")\n",
    "\n",
    "# ì˜ˆì‹œ ì¶œë ¥: train ë¬¸ì„œ 0ì˜ top-k í›„ë³´\n",
    "print(\"Example (first document):\", select_topk_candidates(sims[0], min_k=2, max_k=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3fc7264-de9d-43df-b757-348d8e5d4fd5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T12:22:31.137356Z",
     "iopub.status.busy": "2025-11-30T12:22:31.137065Z",
     "iopub.status.idle": "2025-11-30T12:22:32.721175Z",
     "shell.execute_reply": "2025-11-30T12:22:32.720610Z",
     "shell.execute_reply.started": "2025-11-30T12:22:31.137337Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating silver labels using simple Top-k scheme with BERT sims...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building silver labels (Top-k, BERT): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29487/29487 [00:01<00:00, 18766.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "silver_labels_v1 shape: (29487, 531)\n",
      "avg positives per doc: 3.0\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# 6. SIMPLE SILVER LABEL GENERATION (Top-k ë°©ì‹, BERT sims)\n",
    "# ================================================\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def build_silver_labels_topk(sims, min_k=2, max_k=3):\n",
    "    \"\"\"\n",
    "    sims : (N_train, NUM_CLASSES) BERT-based cosine similarity matrix\n",
    "           ex) sims[i, j] = cos(doc_i, class_j)\n",
    "    return:\n",
    "        silver_y : (N, C) float32 (0/1 labels, Top-k)\n",
    "    \"\"\"\n",
    "    N, C = sims.shape\n",
    "    silver_y = np.zeros((N, C), dtype=np.float32)\n",
    "\n",
    "    for i in tqdm(range(N), desc=\"Building silver labels (Top-k, BERT)\"):\n",
    "        # --- A. ì•ˆì „í•˜ê²Œ row ë³µì‚¬ ---\n",
    "        row = np.array(sims[i], dtype=np.float32).copy()   # shape: (C,)\n",
    "\n",
    "        # --- B. shape ê²€ì‚¬ ---\n",
    "        assert row.shape[0] == C, \\\n",
    "            f\"Error: sims row shape {row.shape}, expected ({C},). Check sims axis order.\"\n",
    "\n",
    "        # --- C. row ê°’ì´ ì „ë¶€ 0ì¸ì§€ ì²´í¬ (ì´ìƒì¹˜ ëŒ€ë¹„ìš©) ---\n",
    "        # BERT ì½”ì‚¬ì¸ ìœ ì‚¬ë„ëŠ” ì¼ë°˜ì ìœ¼ë¡œ -1~1 ì‚¬ì´ ê°’ì´ë¼\n",
    "        # max==0 ì¸ ê²½ìš°ëŠ” ê±°ì˜ ì—†ì§€ë§Œ, NaN/ì „ì²˜ë¦¬ ë¬¸ì œ ëŒ€ë¹„ìš©\n",
    "        if np.nanmax(row) == 0:\n",
    "            # similarity ì •ë³´ê°€ ì „í˜€ ì—†ë‹¤ë©´ ê·¸ëƒ¥ ë§ˆì§€ë§‰ max_kê°œ ì¸ë±ìŠ¤ë¥¼ fallbackìœ¼ë¡œ ì‚¬ìš©\n",
    "            topk_idx = np.arange(C)[-max_k:]\n",
    "        else:\n",
    "            # NaNì„ 0ìœ¼ë¡œ ì¹˜í™˜\n",
    "            row = np.nan_to_num(row, nan=0.0)\n",
    "\n",
    "            # --- D. ì •ìƒ top-k ì„ íƒ ---\n",
    "            # argsort ascending â†’ ìƒìœ„ max_k â†’ ë‚´ë¦¼ì°¨ìˆœ\n",
    "            topk_idx = row.argsort()[-max_k:][::-1]\n",
    "\n",
    "        # --- E. min_k ë³´ì¥ ---\n",
    "        if len(topk_idx) < min_k:\n",
    "            topk_idx = row.argsort()[-min_k:][::-1]\n",
    "\n",
    "        # --- F. silver label ê¸°ë¡ ---\n",
    "        silver_y[i, topk_idx] = 1.0\n",
    "\n",
    "    return silver_y\n",
    "\n",
    "\n",
    "print(\"Generating silver labels using simple Top-k scheme with BERT sims...\")\n",
    "\n",
    "silver_labels_v1 = build_silver_labels_topk(\n",
    "    sims,        # âœ… BERT ê¸°ë°˜ similarity í–‰ë ¬ ì‚¬ìš©\n",
    "    min_k=2,\n",
    "    max_k=3\n",
    ")\n",
    "\n",
    "print(\"silver_labels_v1 shape:\", silver_labels_v1.shape)\n",
    "print(\"avg positives per doc:\", silver_labels_v1.sum(axis=1).mean())\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "569ac2ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T12:24:07.960591Z",
     "iopub.status.busy": "2025-11-30T12:24:07.960275Z",
     "iopub.status.idle": "2025-11-30T12:24:07.986262Z",
     "shell.execute_reply": "2025-11-30T12:24:07.985661Z",
     "shell.execute_reply.started": "2025-11-30T12:24:07.960573Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_hat built: torch.Size([531, 531])\n",
      "BERT-based GNN classifier definitions loaded.\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# 7. LABEL-GCN + DOCUMENT-CLASS CLASSIFIER (BERT ê¸°ë°˜)\n",
    "# ================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (1) Build normalized adjacency A_hat for GCN\n",
    "# ------------------------------------------------\n",
    "def build_normalized_adj(num_classes, edges):\n",
    "    \"\"\"\n",
    "    edges: [(parent, child), ...]\n",
    "    ì¶œë ¥: A_hat (torch.FloatTensor, [C, C])\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    A = np.zeros((num_classes, num_classes), dtype=np.float32)\n",
    "\n",
    "    # parent-child ì—°ê²°ì„ ì–‘ë°©í–¥ìœ¼ë¡œ ë„£ê¸°\n",
    "    for p, c in edges:\n",
    "        A[p, c] = 1.0\n",
    "        A[c, p] = 1.0\n",
    "\n",
    "    # self-loop\n",
    "    np.fill_diagonal(A, 1.0)\n",
    "\n",
    "    # D^{-1/2} * A * D^{-1/2}\n",
    "    deg = A.sum(axis=1)\n",
    "    deg_inv_sqrt = np.power(deg, -0.5)\n",
    "    deg_inv_sqrt[np.isinf(deg_inv_sqrt)] = 0.0\n",
    "    D_inv_sqrt = np.diag(deg_inv_sqrt)\n",
    "\n",
    "    A_hat = D_inv_sqrt @ A @ D_inv_sqrt\n",
    "    return torch.from_numpy(A_hat).float()\n",
    "\n",
    "\n",
    "A_hat = build_normalized_adj(NUM_CLASSES, edges).to(device)\n",
    "print(\"A_hat built:\", A_hat.shape)\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (2) Label Encoder: GCN (BERT ì„ë² ë”© ì…ë ¥)\n",
    "# ------------------------------------------------\n",
    "class LabelGCN(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim=256, num_layers=2, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        dims = [in_dim] + [hidden_dim] * num_layers\n",
    "        self.linears = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            self.linears.append(nn.Linear(dims[i], dims[i+1]))\n",
    "\n",
    "    def forward(self, A_hat, H):\n",
    "        \"\"\"\n",
    "        H: [C, in_dim]  (í´ë˜ìŠ¤ ì´ˆê¸° ì„ë² ë”©, ì—¬ê¸°ì„œëŠ” BERT class_emb)\n",
    "        \"\"\"\n",
    "        x = H  # [C, in_dim]\n",
    "        for i, lin in enumerate(self.linears):\n",
    "            x = A_hat @ x          # GCN aggregation\n",
    "            x = lin(x)\n",
    "            if i < self.num_layers - 1:\n",
    "                x = F.relu(x)\n",
    "                x = self.dropout(x)\n",
    "        return x  # [C, hidden_dim]\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (3) ì „ì²´ classifier: BERT doc â†’ projection â†’ dot with label GCN\n",
    "# ------------------------------------------------\n",
    "class TaxonomyClassifier(nn.Module):\n",
    "    def __init__(self, emb_dim, hidden_dim=256):\n",
    "        \"\"\"\n",
    "        emb_dim   : BERT ì„ë² ë”© ì°¨ì› (ì˜ˆ: 768)\n",
    "        hidden_dim: GCN / ë¬¸ì„œ ì„ë² ë”© ê³µí†µ latent ì°¨ì›\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # ë¬¸ì„œ ì„ë² ë”© projection: BERT_dim â†’ hidden_dim\n",
    "        self.doc_proj = nn.Linear(emb_dim, hidden_dim, bias=False)\n",
    "        \n",
    "        # ë¼ë²¨ GCN: ì´ˆê¸° í”¼ì²˜ë„ BERT_dim â†’ hidden_dim\n",
    "        self.label_gcn = LabelGCN(\n",
    "            in_dim=emb_dim,     # label initial features = BERT class_emb\n",
    "            hidden_dim=hidden_dim,\n",
    "            num_layers=2,\n",
    "            dropout=0.5\n",
    "        )\n",
    "\n",
    "    def forward(self, doc_feats, label_feats, A_hat):\n",
    "        \"\"\"\n",
    "        doc_feats   : [N, emb_dim]   BERT ë¬¸ì„œ ì„ë² ë”© (doc_emb_train/test)\n",
    "        label_feats : [C, emb_dim]   BERT í´ë˜ìŠ¤ ì„ë² ë”© (class_emb)\n",
    "        A_hat       : [C, C]         taxonomy normalized adjacency\n",
    "        \"\"\"\n",
    "        # 1) Document embedding\n",
    "        doc_emb = self.doc_proj(doc_feats)             # [N, d]\n",
    "\n",
    "        # 2) Label embedding via GCN\n",
    "        label_emb = self.label_gcn(A_hat, label_feats) # [C, d]\n",
    "\n",
    "        # 3) Matching score (bilinearì˜ ë‹¨ìˆœ ë²„ì „ = dot product)\n",
    "        logits = doc_emb @ label_emb.T                 # [N, C]\n",
    "\n",
    "        return logits, doc_emb, label_emb\n",
    "\n",
    "\n",
    "print(\"BERT-based GNN classifier definitions loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d716f2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T12:25:55.598480Z",
     "iopub.status.busy": "2025-11-30T12:25:55.598220Z",
     "iopub.status.idle": "2025-11-30T12:25:57.101261Z",
     "shell.execute_reply": "2025-11-30T12:25:57.100730Z",
     "shell.execute_reply.started": "2025-11-30T12:25:55.598462Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 29487\n",
      "Batch size: 64\n",
      "\n",
      "===== ROUND 1 â€” EPOCH 1 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 224/461 [00:00<00:00, 600.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 100 â€” loss: 0.0307\n",
      "  Step 200 â€” loss: 0.0290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 422/461 [00:00<00:00, 640.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 300 â€” loss: 0.0287\n",
      "  Step 400 â€” loss: 0.0248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 461/461 [00:00<00:00, 601.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] AVG LOSS = 0.046713\n",
      "\n",
      "===== ROUND 1 â€” EPOCH 2 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  14%|â–ˆâ–        | 66/461 [00:00<00:00, 655.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 100 â€” loss: 0.0234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 198/461 [00:00<00:00, 645.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 200 â€” loss: 0.0229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 328/461 [00:00<00:00, 643.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 300 â€” loss: 0.0224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 394/461 [00:00<00:00, 645.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 400 â€” loss: 0.0220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 461/461 [00:00<00:00, 639.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] AVG LOSS = 0.022487\n",
      "\n",
      "Round 1 training finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# 8. ROUND 1 TRAINING WITH SILVER LABELS v1 (BERT version)\n",
    "# ================================================\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (1) Dataset: BERT doc_emb + silver labels\n",
    "# ------------------------------------------------\n",
    "class SilverDataset(Dataset):\n",
    "    def __init__(self, doc_emb_tensor, y_np, pid_list=None):\n",
    "        \"\"\"\n",
    "        doc_emb_tensor : torch.FloatTensor [N, H]  BERT ë¬¸ì„œ ì„ë² ë”©\n",
    "        y_np           : numpy array [N, C]        silver_labels_v1\n",
    "        pid_list       : ì˜µì…˜\n",
    "        \"\"\"\n",
    "        self.X = doc_emb_tensor        # already dense\n",
    "        self.y = torch.from_numpy(y_np).float()\n",
    "        self.pid_list = pid_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (2) Dataset / DataLoader\n",
    "# ------------------------------------------------\n",
    "train_dataset = SilverDataset(\n",
    "    doc_emb_train,        # [N_train, emb_dim]\n",
    "    silver_labels_v1,     # [N_train, 531]\n",
    "    pid_list_train\n",
    ")\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(\"Train dataset size:\", len(train_dataset))\n",
    "print(\"Batch size:\", batch_size)\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (3) ëª¨ë¸ / optimizer / loss ì •ì˜\n",
    "# ------------------------------------------------\n",
    "emb_dim = doc_emb_train.shape[1]   # ë³´í†µ 768\n",
    "hidden_dim = 256\n",
    "\n",
    "model = TaxonomyClassifier(\n",
    "    emb_dim=emb_dim,\n",
    "    hidden_dim=hidden_dim\n",
    ").to(device)\n",
    "\n",
    "# Label GCN initial features = BERT class_emb\n",
    "label_feats = class_emb.to(device)   # [C, emb_dim]\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (4) Training loop  (Round 1)\n",
    "# ------------------------------------------------\n",
    "num_epochs = 2\n",
    "logging_interval = 100   # ë§¤ 100ìŠ¤í…ë§ˆë‹¤ loss ì¶œë ¥\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    step = 0\n",
    "\n",
    "    print(f\"\\n===== ROUND 1 â€” EPOCH {epoch} =====\")\n",
    "    for batch_x, batch_y in tqdm(train_loader, desc=f\"Epoch {epoch}\"):\n",
    "\n",
    "        batch_x = batch_x.to(device)    # [B, emb_dim]\n",
    "        batch_y = batch_y.to(device)    # [B, 531]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits, doc_emb, label_emb = model(batch_x, label_feats, A_hat)\n",
    "\n",
    "        loss = criterion(logits, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * batch_x.size(0)\n",
    "        step += 1\n",
    "\n",
    "        # ğŸ”¥ ìŠ¤í…ë³„ loss ëª¨ë‹ˆí„°ë§\n",
    "        if step % logging_interval == 0:\n",
    "            print(f\"  Step {step} â€” loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / len(train_dataset)\n",
    "    print(f\"[Epoch {epoch}] AVG LOSS = {avg_loss:.6f}\")\n",
    "\n",
    "print(\"\\nRound 1 training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "655c7443",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T12:34:34.504752Z",
     "iopub.status.busy": "2025-11-30T12:34:34.504345Z",
     "iopub.status.idle": "2025-11-30T12:34:34.729996Z",
     "shell.execute_reply": "2025-11-30T12:34:34.729377Z",
     "shell.execute_reply.started": "2025-11-30T12:34:34.504732Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting train probabilities (Round 1)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting Round 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 461/461 [00:00<00:00, 2784.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probs_train shape: (29487, 531)\n",
      "probs range: 1.481398e-15 0.99778086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "silver_labels_v2 shape: (29487, 531)\n",
      "avg positives per doc (v1): 3.0\n",
      "avg positives per doc (v2): 0.6142707\n",
      "Self-training Round 2 labels ready.\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# 9. SELF-TRAINING: BUILD SILVER LABELS v2 FROM ROUND 1 MODEL (BERT)\n",
    "# ================================================\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (1) Round 1 prediction í•¨ìˆ˜ (train ì „ì²´ predict, BERT ë²„ì „)\n",
    "# ------------------------------------------------\n",
    "def predict_train_probs(model, doc_emb_tensor, label_feats, A_hat, batch_size=64):\n",
    "    \"\"\"\n",
    "    Train ì „ì²´ ë¬¸ì„œì— ëŒ€í•´ í™•ë¥  ì˜ˆì¸¡ì„ ë°˜í™˜í•œë‹¤.\n",
    "    doc_emb_tensor : torch.FloatTensor [N_train, emb_dim] (doc_emb_train)\n",
    "    label_feats    : torch.FloatTensor [C, emb_dim] (class_emb)\n",
    "    A_hat          : [C, C] normalized adjacency\n",
    "    return:\n",
    "        probs : numpy array (N_train, NUM_CLASSES)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    N = doc_emb_tensor.size(0)\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, N, batch_size), desc=\"Predicting Round 1\"):\n",
    "            batch_x = doc_emb_tensor[i : i+batch_size].to(device)  # [B, emb_dim]\n",
    "\n",
    "            logits, _, _ = model(batch_x, label_feats, A_hat)      # [B, C]\n",
    "            probs = torch.sigmoid(logits).cpu().numpy()\n",
    "\n",
    "            all_probs.append(probs)\n",
    "\n",
    "    return np.vstack(all_probs)\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (2) Round 1 ì „ì²´ ì˜ˆì¸¡ ìˆ˜í–‰\n",
    "# ------------------------------------------------\n",
    "print(\"Predicting train probabilities (Round 1)...\")\n",
    "\n",
    "probs_train = predict_train_probs(\n",
    "    model,\n",
    "    doc_emb_train,   # âœ… BERT ë¬¸ì„œ ì„ë² ë”© ì‚¬ìš©\n",
    "    label_feats,     # class_emb.to(device)\n",
    "    A_hat,\n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "print(\"probs_train shape:\", probs_train.shape)  # (N_train, 531)\n",
    "print(\"probs range:\", probs_train.min(), probs_train.max())\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (3) Silver labels v2 ìƒì„±\n",
    "#     HIGH = 0.7  â†’ confident positive\n",
    "#     LOW  = 0.3  â†’ confident negative\n",
    "# ------------------------------------------------\n",
    "def build_silver_labels_v2(probs, silver_v1, high=0.7, low=0.3):\n",
    "    \"\"\"\n",
    "    probs     : round 1 predicted probabilities (N, C)\n",
    "    silver_v1 : previous silver labels (N, C)\n",
    "    return:\n",
    "        silver_v2 (N, C)\n",
    "    \"\"\"\n",
    "    silver_v2 = silver_v1.copy()\n",
    "\n",
    "    pos_mask = probs >= high    # ëª¨ë¸ì´ ê°•í•˜ê²Œ 1ì´ë¼ ë³´ëŠ” ìœ„ì¹˜\n",
    "    neg_mask = probs <= low     # ëª¨ë¸ì´ ê°•í•˜ê²Œ 0ì´ë¼ ë³´ëŠ” ìœ„ì¹˜\n",
    "\n",
    "    silver_v2[pos_mask] = 1.0\n",
    "    silver_v2[neg_mask] = 0.0\n",
    "\n",
    "    return silver_v2\n",
    "\n",
    "\n",
    "silver_labels_v2 = build_silver_labels_v2(\n",
    "    probs_train,\n",
    "    silver_labels_v1,\n",
    "    high=0.7,\n",
    "    low=0.3\n",
    ")\n",
    "\n",
    "print(\"silver_labels_v2 shape:\", silver_labels_v2.shape)\n",
    "print(\"avg positives per doc (v1):\", silver_labels_v1.sum(axis=1).mean())\n",
    "print(\"avg positives per doc (v2):\", silver_labels_v2.sum(axis=1).mean())\n",
    "print(\"Self-training Round 2 labels ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e07bbb04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T12:35:47.560298Z",
     "iopub.status.busy": "2025-11-30T12:35:47.560017Z",
     "iopub.status.idle": "2025-11-30T12:35:49.066698Z",
     "shell.execute_reply": "2025-11-30T12:35:49.065995Z",
     "shell.execute_reply.started": "2025-11-30T12:35:47.560278Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 2 train dataset size: 29487\n",
      "Round 2 batch size: 64\n",
      "\n",
      "===== ROUND 2 â€” EPOCH 1 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Round 2] Epoch 1:  13%|â–ˆâ–        | 59/461 [00:00<00:00, 582.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Round 2] Step 100 â€” loss: 0.001564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Round 2] Epoch 1:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 180/461 [00:00<00:00, 596.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Round 2] Step 200 â€” loss: 0.001911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Round 2] Epoch 1:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 302/461 [00:00<00:00, 600.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Round 2] Step 300 â€” loss: 0.002385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Round 2] Epoch 1:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 363/461 [00:00<00:00, 602.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Round 2] Step 400 â€” loss: 0.001715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Round 2] Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 461/461 [00:00<00:00, 598.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round 2 - Epoch 1] AVG LOSS = 0.002332\n",
      "\n",
      "===== ROUND 2 â€” EPOCH 2 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Round 2] Epoch 2:  14%|â–ˆâ–        | 63/461 [00:00<00:00, 628.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Round 2] Step 100 â€” loss: 0.001546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Round 2] Epoch 2:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 194/461 [00:00<00:00, 634.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Round 2] Step 200 â€” loss: 0.001604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Round 2] Epoch 2:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 324/461 [00:00<00:00, 643.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Round 2] Step 300 â€” loss: 0.001239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Round 2] Epoch 2:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 389/461 [00:00<00:00, 635.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Round 2] Step 400 â€” loss: 0.001220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Round 2] Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 461/461 [00:00<00:00, 635.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round 2 - Epoch 2] AVG LOSS = 0.001863\n",
      "\n",
      "Round 2 training finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# 10. ROUND 2 TRAINING WITH SILVER LABELS v2 (BERT version)\n",
    "# ================================================\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "# SilverDataset(BERTìš©)ëŠ” 8ë²ˆ ì…€ì—ì„œ ì´ë¯¸ ì •ì˜ë˜ì–´ ìˆìŒ:\n",
    "# class SilverDataset(Dataset):\n",
    "#     def __init__(self, doc_emb_tensor, y_np, pid_list=None):\n",
    "#         ...\n",
    "#     def __len__(self):\n",
    "#         ...\n",
    "#     def __getitem__(self, idx):\n",
    "#         ...\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (1) Round 2ìš© Dataset / DataLoader ìƒì„±\n",
    "# ------------------------------------------------\n",
    "train_dataset_v2 = SilverDataset(\n",
    "    doc_emb_train,    # âœ… BERT ë¬¸ì„œ ì„ë² ë”©\n",
    "    silver_labels_v2, # âœ… Round 2 silver labels\n",
    "    pid_list_train\n",
    ")\n",
    "\n",
    "batch_size_round2 = 64\n",
    "train_loader_v2 = DataLoader(\n",
    "    train_dataset_v2,\n",
    "    batch_size=batch_size_round2,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(\"Round 2 train dataset size:\", len(train_dataset_v2))\n",
    "print(\"Round 2 batch size:\", batch_size_round2)\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (2) Optimizer / Loss ì¬ì„¤ì • (ëª¨ë¸ì€ Round 1ì—ì„œ ì´ì–´ì„œ ì‚¬ìš©)\n",
    "# ------------------------------------------------\n",
    "criterion_round2 = nn.BCEWithLogitsLoss()\n",
    "optimizer_round2 = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (3) Round 2 Training Loop\n",
    "# ------------------------------------------------\n",
    "num_epochs_round2 = 2          # Round 2ì—ì„œë„ 1~2 epoch ì •ë„ ì¶”ì²œ\n",
    "logging_interval = 100         # stepë§ˆë‹¤ loss ëª¨ë‹ˆí„°ë§ ê°„ê²©\n",
    "\n",
    "for epoch in range(1, num_epochs_round2 + 1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    step = 0\n",
    "\n",
    "    print(f\"\\n===== ROUND 2 â€” EPOCH {epoch} =====\")\n",
    "    for batch_x, batch_y in tqdm(train_loader_v2, desc=f\"[Round 2] Epoch {epoch}\"):\n",
    "        batch_x = batch_x.to(device)   # [B, emb_dim]\n",
    "        batch_y = batch_y.to(device)   # [B, C]\n",
    "\n",
    "        optimizer_round2.zero_grad()\n",
    "\n",
    "        # label_feats, A_hat, model ì€ Round 1ì—ì„œ ì´ë¯¸ ì •ì˜ëœ ê²ƒì„ ì‚¬ìš©\n",
    "        logits, doc_emb, label_emb = model(batch_x, label_feats, A_hat)  # [B, C]\n",
    "        loss = criterion_round2(logits, batch_y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer_round2.step()\n",
    "\n",
    "        total_loss += loss.item() * batch_x.size(0)\n",
    "        step += 1\n",
    "\n",
    "        # ğŸ” ì¤‘ê°„ loss ëª¨ë‹ˆí„°ë§\n",
    "        if step % logging_interval == 0:\n",
    "            print(f\"  [Round 2] Step {step} â€” loss: {loss.item():.6f}\")\n",
    "\n",
    "    avg_loss = total_loss / len(train_dataset_v2)\n",
    "    print(f\"[Round 2 - Epoch {epoch}] AVG LOSS = {avg_loss:.6f}\")\n",
    "\n",
    "print(\"\\nRound 2 training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20ce8f55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T12:39:02.151727Z",
     "iopub.status.busy": "2025-11-30T12:39:02.151433Z",
     "iopub.status.idle": "2025-11-30T12:39:02.835924Z",
     "shell.execute_reply": "2025-11-30T12:39:02.835383Z",
     "shell.execute_reply.started": "2025-11-30T12:39:02.151707Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting test probabilities with Round 2 model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting on test: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 308/308 [00:00<00:00, 3363.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probs_test shape: (19658, 531)\n",
      "probs_test range: 2.6146454e-37 1.0\n",
      "\n",
      "Computing BERT-based docâ€“class sims for test set...\n",
      "sims_test shape: (19658, 531)\n",
      "sims_test range: 0.25432742 0.8487386\n",
      "\n",
      "final_scores shape: (19658, 531)\n",
      "final_scores range: 0.101731725 0.92894423\n",
      "\n",
      "Generating final predictions for submission.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring test instances: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19658/19658 [00:00<00:00, 40681.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label count per doc:\n",
      "  1 labels : 9461 docs\n",
      "  2 labels : 10197 docs\n",
      "\n",
      "Submission file saved to: submission.csv\n",
      "Total samples: 19658, Classes per sample: 2-3 (dynamic)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# 11. FINAL TEST PREDICTION + SUBMISSION (BERT + GCN)\n",
    "# ================================================\n",
    "import csv\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "SUBMISSION_PATH = \"submission.csv\"  # output file\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (1) Test ì „ì²´ì— ëŒ€í•œ í™•ë¥  ì˜ˆì¸¡ (Round 2 model)\n",
    "# ------------------------------------------------\n",
    "def predict_test_probs(model, doc_emb_tensor, label_feats, A_hat, batch_size=64):\n",
    "    \"\"\"\n",
    "    doc_emb_tensor : torch.FloatTensor [N_test, emb_dim] (doc_emb_test)\n",
    "    label_feats    : torch.FloatTensor [C, emb_dim] (class_emb)\n",
    "    A_hat          : [C, C]\n",
    "    return:\n",
    "        probs_test : numpy array [N_test, NUM_CLASSES]\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    N = doc_emb_tensor.size(0)\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, N, batch_size), desc=\"Predicting on test\"):\n",
    "            batch_x = doc_emb_tensor[i : i+batch_size].to(device)  # [B, emb_dim]\n",
    "\n",
    "            logits, _, _ = model(batch_x, label_feats, A_hat)      # [B, C]\n",
    "            probs = torch.sigmoid(logits).cpu().numpy()\n",
    "            all_probs.append(probs)\n",
    "\n",
    "    return np.vstack(all_probs)\n",
    "\n",
    "\n",
    "print(\"Predicting test probabilities with Round 2 model...\")\n",
    "\n",
    "probs_test = predict_test_probs(\n",
    "    model,\n",
    "    doc_emb_test,   # âœ… BERT test ë¬¸ì„œ ì„ë² ë”©\n",
    "    label_feats,    # âœ… class_emb.to(device)\n",
    "    A_hat,\n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "print(\"probs_test shape:\", probs_test.shape)\n",
    "print(\"probs_test range:\", probs_test.min(), probs_test.max())\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (2) BERT cosine similarity ê¸°ë°˜ sims_test ê³„ì‚°\n",
    "# ------------------------------------------------\n",
    "print(\"\\nComputing BERT-based docâ€“class sims for test set...\")\n",
    "\n",
    "# doc_emb_test, class_emb ë‘˜ ë‹¤ ê°™ì€ ê³µê°„(BERT)ì—ì„œ normalize\n",
    "doc_emb_test_norm = F.normalize(doc_emb_test, p=2, dim=1)        # [N_test, H]\n",
    "class_emb_norm    = F.normalize(class_emb,    p=2, dim=1)        # [C, H]\n",
    "\n",
    "# torch matmul â†’ numpy\n",
    "sims_test = torch.matmul(\n",
    "    doc_emb_test_norm.to(device),\n",
    "    class_emb_norm.to(device).T\n",
    ").cpu().numpy().astype(\"float32\")                                # [N_test, C]\n",
    "\n",
    "print(\"sims_test shape:\", sims_test.shape)\n",
    "print(\"sims_test range:\", sims_test.min(), sims_test.max())\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (3) MODEL + SIMS BLENDING\n",
    "# ------------------------------------------------\n",
    "lambda_model = 0.6\n",
    "lambda_sims  = 0.4\n",
    "\n",
    "final_scores = lambda_model * probs_test + lambda_sims * sims_test\n",
    "print(\"\\nfinal_scores shape:\", final_scores.shape)\n",
    "print(\"final_scores range:\", final_scores.min(), final_scores.max())\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (4) ë™ì  ë¼ë²¨ ì„ íƒ í•¨ìˆ˜: ë¬¸ì„œë‹¹ 1~3ê°œ (threshold ê¸°ë°˜)\n",
    "# ------------------------------------------------\n",
    "def pick_labels(score_row, min_k=1, max_k=3, threshold=0.5):\n",
    "    \"\"\"\n",
    "    score_row : shape (C,) - í•œ ë¬¸ì„œì˜ ìµœì¢… ì ìˆ˜ ë²¡í„°\n",
    "    min_k     : ìµœì†Œ ë¼ë²¨ ê°œìˆ˜ (ë³´í†µ 1)\n",
    "    max_k     : ìµœëŒ€ ë¼ë²¨ ê°œìˆ˜ (ë³´í†µ 3)\n",
    "    threshold : ì´ ê°’ ì´ìƒë§Œ 'ìœ ë ¥ í›„ë³´'ë¡œ ë¨¼ì € ì„ íƒ\n",
    "    \"\"\"\n",
    "    # 1) threshold ì´ìƒì¸ í´ë˜ìŠ¤ë§Œ í›„ë³´\n",
    "    cand = np.where(score_row >= threshold)[0]\n",
    "\n",
    "    # 2) threshold ë„˜ëŠ” ê²Œ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ â†’ top-1 ê°•ì œ ì„ íƒ\n",
    "    if len(cand) == 0:\n",
    "        top1 = int(np.argmax(score_row))\n",
    "        return [top1]\n",
    "\n",
    "    # 3) í›„ë³´ê°€ ë„ˆë¬´ ë§ìœ¼ë©´ â†’ ì ìˆ˜ ìˆœìœ¼ë¡œ max_kê°œë§Œ ë‚¨ê¸°ê¸°\n",
    "    if len(cand) > max_k:\n",
    "        sorted_idx = cand[np.argsort(score_row[cand])[::-1]]\n",
    "        cand = sorted_idx[:max_k]\n",
    "\n",
    "    # 4) í›„ë³´ê°€ min_kë³´ë‹¤ ì ìœ¼ë©´ â†’ top-scoreì—ì„œ ì±„ì›Œì„œ min_kê¹Œì§€ ë§ì¶”ê¸°\n",
    "    if len(cand) < min_k:\n",
    "        sorted_idx = np.argsort(score_row)[::-1]\n",
    "        for c in sorted_idx:\n",
    "            if c not in cand:\n",
    "                cand = np.append(cand, c)\n",
    "                if len(cand) == min_k:\n",
    "                    break\n",
    "\n",
    "    return list(map(int, cand))\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (5) SUBMISSION ìƒì„± (Kaggle baseline í¬ë§·: id, labels)\n",
    "# ------------------------------------------------\n",
    "all_pids, all_labels = [], []\n",
    "\n",
    "print(\"\\nGenerating final predictions for submission.csv...\")\n",
    "for i, pid in enumerate(tqdm(pid_list_test, desc=\"Scoring test instances\")):\n",
    "    scores = final_scores[i]\n",
    "    labels = pick_labels(\n",
    "        scores,\n",
    "        min_k=MIN_LABELS,   # ë³´í†µ 1\n",
    "        max_k=MAX_LABELS,   # ë³´í†µ 3\n",
    "        threshold=0.5       # í•„ìš”í•˜ë©´ ì¡°ì • (0.4~0.6 ì‚¬ì´ íŠœë‹ ê°€ëŠ¥)\n",
    "    )\n",
    "    labels = sorted(labels)\n",
    "    all_pids.append(pid)\n",
    "    all_labels.append(labels)\n",
    "\n",
    "# ë¼ë²¨ ê°œìˆ˜ ë¶„í¬ í•œë²ˆ ì²´í¬ (ì„ íƒì‚¬í•­, ë””ë²„ê¹…ìš©)\n",
    "lengths = [len(l) for l in all_labels]\n",
    "print(\"\\nLabel count per doc:\")\n",
    "for L in sorted(set(lengths)):\n",
    "    print(f\"  {L} labels : {lengths.count(L)} docs\")\n",
    "\n",
    "# --- Save submission file ---\n",
    "with open(SUBMISSION_PATH, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"id\", \"labels\"])   # âœ… Kaggleì´ ìš”êµ¬í•˜ëŠ” ì»¬ëŸ¼ ì´ë¦„\n",
    "\n",
    "    for pid, labels in zip(all_pids, all_labels):\n",
    "        writer.writerow([pid, \",\".join(map(str, labels))])\n",
    "\n",
    "print(f\"\\nSubmission file saved to: {SUBMISSION_PATH}\")\n",
    "print(f\"Total samples: {len(all_pids)}, Classes per sample: {MIN_LABELS}-{MAX_LABELS} (dynamic)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063eab09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c062713b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886663cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084431fe-97fd-442e-9bb7-78f1153f80c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b486e5-1fb7-4e9e-b63a-bc3919ab81ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e97ef5e-5ee1-4bb1-9ccd-d0242241c2b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312fa965-ac9f-4255-a3d7-3466aba485f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7419c2a8-6739-4ed6-b9ee-3eeb108873de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating dummy predictions: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19658/19658 [00:00<00:00, 190266.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy submission file saved to: submission.csv\n",
      "Total samples: 19658, Classes per sample: 1-3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ------------------------\n",
    "# Dummy baseline for Kaggle submission\n",
    "# Generates random multi-label predictions\n",
    "# ------------------------\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Paths ---\n",
    "TEST_DIR = \"Amazon_products/test\"  # modify if needed\n",
    "TEST_CORPUS_PATH = os.path.join(TEST_DIR, \"test_corpus.txt\")  # product_id \\t text\n",
    "SUBMISSION_PATH = \"submission.csv\"  # output file\n",
    "\n",
    "# --- Constants ---\n",
    "NUM_CLASSES = 531  # total number of classes (0â€“530)\n",
    "MIN_LABELS = 1     # minimum number of labels per sample\n",
    "MAX_LABELS = 3     # maximum number of labels per sample\n",
    "\n",
    "# --- Load test corpus ---\n",
    "def load_corpus(path):\n",
    "    \"\"\"Load test corpus into {pid: text} dictionary.\"\"\"\n",
    "    pid2text = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\"\\t\", 1)\n",
    "            if len(parts) == 2:\n",
    "                pid, text = parts\n",
    "                pid2text[pid] = text\n",
    "    return pid2text\n",
    "\n",
    "pid2text_test = load_corpus(TEST_CORPUS_PATH)\n",
    "pid_list_test = list(pid2text_test.keys())\n",
    "\n",
    "# --- Generate random predictions ---\n",
    "all_pids, all_labels = [], []\n",
    "for pid in tqdm(pid_list_test, desc=\"Generating dummy predictions\"):\n",
    "    n_labels = random.randint(MIN_LABELS, MAX_LABELS)\n",
    "    labels = random.sample(range(NUM_CLASSES), n_labels)\n",
    "    labels = sorted(labels)\n",
    "    all_pids.append(pid)\n",
    "    all_labels.append(labels)\n",
    "\n",
    "# --- Save submission file ---\n",
    "with open(SUBMISSION_PATH, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"pid\", \"labels\"])\n",
    "    for pid, labels in zip(all_pids, all_labels):\n",
    "        writer.writerow([pid, \",\".join(map(str, labels))])\n",
    "\n",
    "print(f\"Dummy submission file saved to: {SUBMISSION_PATH}\")\n",
    "print(f\"Total samples: {len(all_pids)}, Classes per sample: {MIN_LABELS}-{MAX_LABELS}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ESCI Environment (Python 3.9)",
   "language": "python",
   "name": "esci"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
