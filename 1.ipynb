{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 7,
>>>>>>> abbf8fff3daf2b0d2a1bd36649e418d6fb03b845
   "id": "34537235",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T06:51:47.678607Z",
     "iopub.status.busy": "2025-11-15T06:51:47.678308Z",
     "iopub.status.idle": "2025-11-15T06:51:47.946075Z",
     "shell.execute_reply": "2025-11-15T06:51:47.945239Z",
     "shell.execute_reply.started": "2025-11-15T06:51:47.678585Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "\n",
      "== Data path check ==\n",
      "Amazon_products/train/train_corpus.txt -> True\n",
      "Amazon_products/test/test_corpus.txt -> True\n",
      "Amazon_products/classes.txt -> True\n",
      "Amazon_products/class_hierarchy.txt -> True\n",
      "Amazon_products/class_related_keywords.txt -> True\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# 1. REPRODUCIBILITY SETTINGS \n",
    "# ================================================\n",
    "from pathlib import Path\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "# ----- Reproducibility -----\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
<<<<<<< HEAD
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
=======
>>>>>>> abbf8fff3daf2b0d2a1bd36649e418d6fb03b845
    "\n",
    "# ----- Device -----\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# ================================================\n",
    "# 2. DEFAULT PATHS FOR DATASET\n",
    "# ================================================\n",
    "ROOT = Path(\"Amazon_products\")   # dataset root directory\n",
    "\n",
    "# Main corpus\n",
    "TRAIN_CORPUS_PATH = ROOT / \"train\" /  \"train_corpus.txt\"       # pid \\t text\n",
    "TEST_CORPUS_PATH  = ROOT / \"test\" / \"test_corpus.txt\"        # pid \\t text\n",
    "\n",
    "# Taxonomy & class meta\n",
    "CLASSES_PATH      = ROOT / \"classes.txt\"            # class_id \\t class_name\n",
    "HIERARCHY_PATH    = ROOT / \"class_hierarchy.txt\"    # parent_id \\t child_id\n",
    "KEYWORDS_PATH     = ROOT / \"class_related_keywords.txt\"\n",
    "\n",
    "# Constants\n",
    "NUM_CLASSES = 531\n",
<<<<<<< HEAD
    "MIN_LABELS = 2     # minimum number of labels per sample\n",
=======
    "MIN_LABELS = 1     # minimum number of labels per sample\n",
>>>>>>> abbf8fff3daf2b0d2a1bd36649e418d6fb03b845
    "MAX_LABELS = 3     # maximum number of labels per sample\n",
    "\n",
    "# Check paths\n",
    "print(\"\\n== Data path check ==\")\n",
    "for p in [TRAIN_CORPUS_PATH, TEST_CORPUS_PATH,\n",
    "          CLASSES_PATH, HIERARCHY_PATH, KEYWORDS_PATH]:\n",
    "    print(f\"{p} -> {p.exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95583a7e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T06:54:23.373084Z",
     "iopub.status.busy": "2025-11-15T06:54:23.372743Z",
     "iopub.status.idle": "2025-11-15T06:54:23.432999Z",
     "shell.execute_reply": "2025-11-15T06:54:23.432338Z",
     "shell.execute_reply.started": "2025-11-15T06:54:23.373065Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train/test corpus...\n",
      "Train samples: 29487\n",
      "Test samples : 19658\n",
      "Example train sample #0: pid=0, text=omron hem 790it automatic blood pressure monitor with advanced omron health mana...\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# 2. DATA LOADING\n",
    "# ================================================\n",
    "\n",
    "def load_corpus(path):\n",
    "    \"\"\"\n",
    "    Load corpus file (pid \\\\t text) as {pid: text} dictionary.\n",
    "    \"\"\"\n",
    "    pid2text = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\"\\t\", 1)\n",
    "            if len(parts) == 2:\n",
    "                pid, text = parts\n",
    "                pid2text[pid] = text\n",
    "    return pid2text\n",
    "\n",
    "# Load corpus\n",
    "print(\"Loading train/test corpus...\")\n",
    "\n",
    "pid2text_train = load_corpus(TRAIN_CORPUS_PATH)\n",
    "pid2text_test  = load_corpus(TEST_CORPUS_PATH)\n",
    "\n",
    "pid_list_train = list(pid2text_train.keys())\n",
    "pid_list_test  = list(pid2text_test.keys())\n",
    "\n",
    "print(\"Train samples:\", len(pid2text_train))\n",
    "print(\"Test samples :\", len(pid2text_test))\n",
    "\n",
    "# Quick sample check\n",
    "for i, (pid, text) in enumerate(pid2text_train.items()):\n",
    "    print(f\"Example train sample #{i}: pid={pid}, text={text[:80]}...\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "968916cf-d39c-4884-a813-7daeed03f98e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T06:59:04.281602Z",
     "iopub.status.busy": "2025-11-15T06:59:04.281120Z",
     "iopub.status.idle": "2025-11-15T06:59:04.292192Z",
     "shell.execute_reply": "2025-11-15T06:59:04.291680Z",
     "shell.execute_reply.started": "2025-11-15T06:59:04.281582Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading class metadata...\n",
      "Num classes: 531\n",
      "Num edges in taxonomy: 568\n",
      "\n",
      "Example class id: 0\n",
      "Name: grocery_gourmet_food\n",
      "Keywords: ['snacks', 'condiments', 'beverages', 'specialty_foods', 'spices', 'cooking_oils', 'baking_ingredients', 'gourmet_chocolates', 'artisanal_cheeses', 'organic_foods']\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# 3. CLASS METADATA LOADING\n",
    "# ================================================\n",
    "\n",
    "def load_classes(path):\n",
    "    \"\"\"\n",
    "    classes.txt : class_id \\\\t class_name\n",
    "    returns: id2label, label2id\n",
    "    \"\"\"\n",
    "    id2label = {}\n",
    "    label2id = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\"\\t\")\n",
    "            if len(parts) != 2:\n",
    "                continue\n",
    "            cid, name = parts\n",
    "            cid = int(cid)\n",
    "            id2label[cid] = name\n",
    "            label2id[name] = cid\n",
    "    return id2label, label2id\n",
    "\n",
    "\n",
    "def load_hierarchy(path):\n",
    "    \"\"\"\n",
    "    class_hierarchy.txt : parent_id \\\\t child_id\n",
    "    returns: edges (list of tuples)\n",
    "    \"\"\"\n",
    "    edges = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\"\\t\")\n",
    "            if len(parts) != 2:\n",
    "                continue\n",
    "            p, c = map(int, parts)\n",
    "            edges.append((p, c))\n",
    "    return edges\n",
    "\n",
    "\n",
    "def load_keywords(path, label2id):\n",
    "    \"\"\"\n",
    "    class_related_keywords.txt : CLASS_NAME: kw1, kw2,...\n",
    "    returns: {class_id: [kws]}\n",
    "    \"\"\"\n",
    "    d = {cid: [] for cid in label2id.values()}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if \":\" not in line:\n",
    "                continue\n",
    "            name, kws = line.strip().split(\":\", 1)\n",
    "            kws = [k.strip() for k in kws.split(\",\") if k.strip()]\n",
    "            if name in label2id:\n",
    "                cid = label2id[name]\n",
    "                d[cid] = kws\n",
    "    return d\n",
    "\n",
    "\n",
    "# ----------------- Load all class meta -----------------\n",
    "print(\"Loading class metadata...\")\n",
    "\n",
    "id2label, label2id = load_classes(CLASSES_PATH)\n",
    "edges = load_hierarchy(HIERARCHY_PATH)\n",
    "label_keywords = load_keywords(KEYWORDS_PATH, label2id)\n",
    "\n",
    "print(\"Num classes:\", len(id2label))\n",
    "print(\"Num edges in taxonomy:\", len(edges))\n",
    "print()\n",
    "\n",
    "# Small check\n",
    "example_id = 0\n",
    "print(\"Example class id:\", example_id)\n",
    "print(\"Name:\", id2label[example_id])\n",
    "print(\"Keywords:\", label_keywords[example_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b383df-383e-4b23-949b-1190afd9564e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T07:00:31.964851Z",
     "iopub.status.busy": "2025-11-15T07:00:31.964552Z",
     "iopub.status.idle": "2025-11-15T07:00:38.686137Z",
     "shell.execute_reply": "2025-11-15T07:00:38.685088Z",
     "shell.execute_reply.started": "2025-11-15T07:00:31.964828Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example class text:\n",
      "grocery gourmet food snacks condiments beverages specialty_foods spices cooking_oils baking_ingredie ...\n",
      "\n",
      "Fitting TF-IDF vectorizer...\n",
      "Vocabulary size: 100000\n",
      "X_train_docs: (29487, 100000)\n",
      "X_test_docs : (19658, 100000)\n",
      "X_class     : (531, 100000)\n",
      "\n",
      "Normalizing TF-IDF vectors...\n",
      "\n",
      "Computing doc–class similarity (cosine)...\n",
      "sims shape: (29487, 531)\n",
      "sims min/max: 0.0 0.627883\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# 4. TF-IDF EMBEDDING + DOC–CLASS SIMILARITY\n",
    "#    (class name vs keyword 분리 활용)\n",
    "# ================================================\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
<<<<<<< HEAD
    "import numpy as np\n",
=======
>>>>>>> abbf8fff3daf2b0d2a1bd36649e418d6fb03b845
    "\n",
    "# ------------------------------------------------\n",
    "# (1) 클래스 이름 텍스트 / 키워드 텍스트 분리 생성\n",
    "# ------------------------------------------------\n",
    "def build_class_name_texts(id2label):\n",
    "    \"\"\"\n",
    "    각 class_id에 대해 클래스 이름만 사용한 텍스트 리스트 생성\n",
    "    \"\"\"\n",
    "    texts = []\n",
    "    for cid in range(NUM_CLASSES):\n",
    "        name = id2label[cid].replace(\"_\", \" \")\n",
    "        texts.append(name)\n",
    "    return texts\n",
    "\n",
    "def build_class_keyword_texts(label_keywords):\n",
    "    \"\"\"\n",
    "    각 class_id에 대해 키워드만 이어붙인 텍스트 리스트 생성\n",
    "    \"\"\"\n",
    "    texts = []\n",
    "    for cid in range(NUM_CLASSES):\n",
    "        kws = \" \".join(label_keywords.get(cid, []))\n",
    "        texts.append(kws if kws else \"\")\n",
    "    return texts\n",
    "\n",
    "class_name_texts = build_class_name_texts(id2label)         # [C]\n",
    "class_kw_texts   = build_class_keyword_texts(label_keywords) # [C]\n",
    "\n",
    "print(\"Example class name text:\", class_name_texts[0])\n",
    "print(\"Example class keyword text:\", class_kw_texts[0])\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (2) TF-IDF vocabulary 학습\n",
    "#     - train 문서 + test 문서 + class name + class keyword 모두 포함\n",
    "# ------------------------------------------------\n",
    "all_texts_for_vocab = (\n",
    "    list(pid2text_train.values())\n",
    "    + list(pid2text_test.values())\n",
    "    + class_name_texts\n",
    "    + class_kw_texts\n",
    ")\n",
    "\n",
    "print(\"\\nFitting TF-IDF vectorizer on docs + class names + keywords...\")\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=100_000,   # 필요에 따라 조정 가능\n",
    "    ngram_range=(1, 2),\n",
<<<<<<< HEAD
    "    min_df=1                # ★ 중요: class 단어 손실 방지\n",
=======
    "    min_df=2\n",
>>>>>>> abbf8fff3daf2b0d2a1bd36649e418d6fb03b845
    ")\n",
    "vectorizer.fit(all_texts_for_vocab)\n",
    "\n",
    "print(\"Vocabulary size:\", len(vectorizer.vocabulary_))\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (3) 실제 TF-IDF 행렬 변환\n",
    "# ------------------------------------------------\n",
    "N_train = len(pid2text_train)\n",
    "N_test  = len(pid2text_test)\n",
    "C       = NUM_CLASSES\n",
    "\n",
    "# 문서 TF-IDF\n",
    "X_train_docs = vectorizer.transform(pid2text_train.values())   # [N_train, V]\n",
    "X_test_docs  = vectorizer.transform(pid2text_test.values())    # [N_test, V]\n",
    "\n",
    "# 클래스 이름 TF-IDF (GNN용 initial feature)\n",
    "X_class_name = vectorizer.transform(class_name_texts)          # [C, V]\n",
    "\n",
    "# 클래스 키워드 TF-IDF (silver label용 보조 sim)\n",
    "X_class_kw   = vectorizer.transform(class_kw_texts)            # [C, V]\n",
    "\n",
    "print(\"X_train_docs:\", X_train_docs.shape)\n",
    "print(\"X_test_docs :\", X_test_docs.shape)\n",
    "print(\"X_class_name:\", X_class_name.shape)\n",
    "print(\"X_class_kw  :\", X_class_kw.shape)\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (4) L2 정규화 (코사인 유사도용)\n",
    "# ------------------------------------------------\n",
    "print(\"\\nNormalizing TF-IDF vectors...\")\n",
    "X_train_norm      = normalize(X_train_docs, axis=1)\n",
    "X_class_name_norm = normalize(X_class_name, axis=1)\n",
    "X_class_kw_norm   = normalize(X_class_kw, axis=1)\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (5) 이름 기반 / 키워드 기반 doc–class 유사도 계산\n",
    "# ------------------------------------------------\n",
    "print(\"\\nComputing doc–class cosine similarities...\")\n",
    "\n",
    "# 이름 기반 similarity\n",
    "sims_name = (X_train_norm @ X_class_name_norm.T).toarray().astype(\"float32\")  # [N_train, C]\n",
    "\n",
    "# 키워드 기반 similarity\n",
    "sims_kw   = (X_train_norm @ X_class_kw_norm.T).toarray().astype(\"float32\")    # [N_train, C]\n",
    "\n",
    "print(\"sims_name shape:\", sims_name.shape, \"| min/max:\", sims_name.min(), sims_name.max())\n",
    "print(\"sims_kw   shape:\", sims_kw.shape,   \"| min/max:\", sims_kw.min(),   sims_kw.max())\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (6) 두 채널을 가중합하여 최종 similarity 생성\n",
    "#     - alpha: class name 비중\n",
    "#     - beta : keyword 비중\n",
    "# ------------------------------------------------\n",
<<<<<<< HEAD
    "alpha = 0.9  # class name 중요도\n",
    "beta  = 0.1  # keyword 중요도\n",
=======
    "alpha = 0.7  # class name 중요도\n",
    "beta  = 0.3  # keyword 중요도\n",
>>>>>>> abbf8fff3daf2b0d2a1bd36649e418d6fb03b845
    "\n",
    "sims = alpha * sims_name + beta * sims_kw   # [N_train, C]\n",
    "\n",
    "print(\"\\nFinal sims shape:\", sims.shape)\n",
    "print(\"Final sims min/max:\", sims.min(), sims.max())\n",
<<<<<<< HEAD
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (7) Document-wise Normalization (silver label 품질 핵심)\n",
    "# ------------------------------------------------\n",
    "\n",
    "def zscore_norm(sims):\n",
    "    \"\"\"\n",
    "    Document-wise z-score normalization.\n",
    "    sims: numpy array [N, C]\n",
    "    \"\"\"\n",
    "    sims_mean = sims.mean(axis=1, keepdims=True)\n",
    "    sims_std  = sims.std(axis=1, keepdims=True) + 1e-8\n",
    "    return (sims - sims_mean) / sims_std\n",
    "\n",
    "def minmax_norm(sims):\n",
    "    \"\"\"\n",
    "    Document-wise min-max normalization.\n",
    "    sims: numpy array [N, C]\n",
    "    \"\"\"\n",
    "    sims_min = sims.min(axis=1, keepdims=True)\n",
    "    sims_max = sims.max(axis=1, keepdims=True)\n",
    "    denom = (sims_max - sims_min) + 1e-8\n",
    "    return (sims - sims_min) / denom\n",
    "\n",
    "# ---- Step A: z-score normalization\n",
    "sims_z = zscore_norm(sims)\n",
    "\n",
    "# ---- Step B: positive clipping\n",
    "sims_z = np.maximum(sims_z, 0)\n",
    "\n",
    "# ---- Step C: Min-Max normalization (0~1)\n",
    "sims_norm = minmax_norm(sims_z)\n",
    "\n",
    "print(\"\\nNormalized sims:\")\n",
    "print(\"sims_norm shape:\", sims_norm.shape)\n",
    "print(\"sims_norm min/max:\", sims_norm.min(), sims_norm.max())\n",
=======
>>>>>>> abbf8fff3daf2b0d2a1bd36649e418d6fb03b845
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330b238d-d0cb-4e94-a9d0-4e4eab9ed0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# 5. TOP-DOWN CLASS EXPLORATION (TaxoClass style)\n",
    "# ================================================\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (1) adjacency (parent2child, child2parent) 만들기\n",
    "# ------------------------------------------------\n",
    "parent2child = defaultdict(list)\n",
    "child2parent = defaultdict(list)\n",
    "\n",
    "for p, c in edges:   # edges = [(parent, child), ...]\n",
    "    parent2child[p].append(c)\n",
    "    child2parent[c].append(p)\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (2) Top-Down 탐색 함수\n",
    "# ------------------------------------------------\n",
    "def topdown_candidates(doc_idx, sims_row, root=0, max_depth=5, base_k=2):\n",
    "    \"\"\"\n",
    "    문서-클래스 유사도 sims_row 를 이용하여\n",
    "    taxonomy 기반 top-down class 후보를 탐색.\n",
    "\n",
    "    Return:\n",
    "        candidates (set): 후보 class IDs\n",
    "        path_score (dict): 각 class의 path score\n",
    "    \"\"\"\n",
    "    path_score = {root: 1.0}   # Root의 path score = 1\n",
    "    level_nodes = [root]\n",
    "    visited = set([root])\n",
    "\n",
    "    for depth in range(max_depth):\n",
    "        # 현재 level의 child 후보 모으기\n",
    "        cand = []\n",
    "        for node in level_nodes:\n",
    "            for ch in parent2child.get(node, []):\n",
    "                if ch not in visited:\n",
    "                    cand.append(ch)\n",
    "\n",
    "        # 더 이상 확장할 child가 없으면 종료\n",
    "        if not cand:\n",
    "            break\n",
    "\n",
    "        # child path score 계산\n",
    "        for ch in cand:\n",
    "            parents = child2parent.get(ch, [])\n",
    "            if not parents:\n",
    "                continue\n",
    "            # parent 중 path_score*sim 이 가장 높은 parent 선택\n",
    "            ps = max(\n",
    "                path_score[p] * sims_row[ch]\n",
    "                for p in parents\n",
    "                if p in path_score\n",
    "            )\n",
    "            path_score[ch] = ps\n",
    "\n",
    "        # path_score 기준 상위 k개 선택\n",
    "        k = base_k + depth    # depth=0→2, depth=1→3, ...\n",
    "        cand_sorted = sorted(\n",
    "            cand,\n",
    "            key=lambda x: path_score.get(x, 0.0),\n",
    "            reverse=True\n",
    "        )\n",
    "        level_nodes = cand_sorted[:k]\n",
    "\n",
    "        visited.update(level_nodes)\n",
    "\n",
    "    # root 제외\n",
    "    candidates = visited - {root}\n",
    "    return candidates, path_score\n",
    "\n",
    "\n",
    "print(\"Top-Down exploration module loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7ebd6c-ef51-42bc-ae4a-6c7d48a47d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# 6. CORE CLASS MINING + SILVER LABEL v1\n",
    "# ================================================\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (1) Core class 선택을 위한 함수\n",
    "#     - confidence = sims[c] - max(sim(parent/sibling))\n",
    "# ------------------------------------------------\n",
    "def get_core_classes_for_doc(sims_row, candidates, top_m=3, margin=0.02):\n",
    "    \"\"\"\n",
    "    sims_row : shape (NUM_CLASSES,), 해당 문서 d의 sims[d]\n",
    "    candidates : topdown_candidates()로 얻은 후보 class ID 집합\n",
    "    top_m : core class 최대 개수\n",
    "    margin : parent/sibling보다 얼마나 더 커야 core로 인정할지\n",
    "    \"\"\"\n",
    "    confs = []  # (cid, conf) 리스트\n",
    "\n",
    "    for c in candidates:\n",
    "        parents = child2parent.get(c, [])\n",
    "        sibs = set()\n",
    "        for p in parents:\n",
    "            sibs.update(parent2child.get(p, []))\n",
    "        sibs.discard(c)\n",
    "\n",
    "        base_sim = 0.0\n",
    "        idxs = list(parents) + list(sibs)\n",
    "        if idxs:\n",
    "            base_sim = sims_row[idxs].max()\n",
    "\n",
    "        conf = float(sims_row[c] - base_sim)\n",
    "        confs.append((c, conf))\n",
    "\n",
    "    # margin 이상인 클래스만 남기고, confidence 기준 내림차순 정렬\n",
    "    good = [(c, conf) for c, conf in confs if conf > margin]\n",
    "    good.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    core = [c for c, _ in good[:top_m]]\n",
    "    return core\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (2) Silver label v1 생성\n",
    "#     - core + parents(core) = positive\n",
    "# ------------------------------------------------\n",
    "def build_silver_labels_v1(\n",
    "    sims,\n",
    "    max_depth=5,\n",
    "    base_k=2,\n",
    "    top_m=3,\n",
    "    margin=0.02\n",
    "):\n",
    "    \"\"\"\n",
    "    sims: (N_train, NUM_CLASSES) 문서-클래스 similarity\n",
    "    반환: silver_labels_v1 (N_train, NUM_CLASSES) 의 0/1 numpy array\n",
    "    \"\"\"\n",
    "    N_train = sims.shape[0]\n",
    "    silver = np.zeros((N_train, NUM_CLASSES), dtype=np.float32)\n",
    "\n",
    "    for i in tqdm(range(N_train), desc=\"Building silver labels v1\"):\n",
    "        sims_row = sims[i]\n",
    "\n",
    "        # 1) Top-Down으로 후보 class 탐색\n",
    "        candidates, _ = topdown_candidates(\n",
    "            doc_idx=i,\n",
    "            sims_row=sims_row,\n",
    "            root=0,\n",
    "            max_depth=max_depth,\n",
    "            base_k=base_k,\n",
    "        )\n",
    "        if not candidates:\n",
    "            continue\n",
    "\n",
    "        # 2) core class 선택\n",
    "        cores = get_core_classes_for_doc(\n",
    "            sims_row,\n",
    "            candidates,\n",
    "            top_m=top_m,\n",
    "            margin=margin,\n",
    "        )\n",
    "        if not cores:\n",
    "            continue\n",
    "\n",
    "        # 3) positive set = core + parents(core)\n",
    "        pos = set()\n",
    "        for c in cores:\n",
    "            pos.add(c)\n",
    "            for p in child2parent.get(c, []):\n",
    "                pos.add(p)\n",
    "\n",
    "        # 4) positive 라벨을 1로 설정 (나머지는 0 = negative 취급)\n",
    "        for c in pos:\n",
    "            silver[i, c] = 1.0\n",
    "\n",
    "    return silver\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (3) 실제 silver_labels_v1 생성\n",
    "# ------------------------------------------------\n",
    "print(\"Generating silver_labels_v1 using sims (TF-IDF + name/keyword)...\")\n",
    "\n",
    "silver_labels_v1 = build_silver_labels_v1(\n",
    "    sims,\n",
    "    max_depth=5,   # 필요하면 조정 가능\n",
    "    base_k=2,\n",
    "    top_m=3,\n",
    "    margin=0.02,\n",
    ")\n",
    "\n",
    "print(\"silver_labels_v1 shape:\", silver_labels_v1.shape)\n",
    "print(\"avg positives per doc:\", silver_labels_v1.sum(axis=1).mean())\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569ac2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# 7. LABEL-GCN + DOCUMENT-CLASS CLASSIFIER\n",
    "# ================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (1) Build normalized adjacency A_hat for GCN\n",
    "# ------------------------------------------------\n",
    "def build_normalized_adj(num_classes, edges):\n",
    "    \"\"\"\n",
    "    edges: [(parent, child), ...]\n",
    "    출력: A_hat (torch.FloatTensor, [C,C])\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    A = np.zeros((num_classes, num_classes), dtype=np.float32)\n",
    "\n",
    "    # parent-child 연결을 양방향으로 넣기\n",
    "    for p, c in edges:\n",
    "        A[p, c] = 1.0\n",
    "        A[c, p] = 1.0\n",
    "\n",
    "    # self-loop\n",
    "    np.fill_diagonal(A, 1.0)\n",
    "\n",
    "    # D^{-1/2} * A * D^{-1/2}\n",
    "    deg = A.sum(axis=1)\n",
    "    deg_inv_sqrt = np.power(deg, -0.5)\n",
    "    deg_inv_sqrt[np.isinf(deg_inv_sqrt)] = 0.0\n",
    "    D_inv_sqrt = np.diag(deg_inv_sqrt)\n",
    "\n",
    "    A_hat = D_inv_sqrt @ A @ D_inv_sqrt\n",
    "    return torch.from_numpy(A_hat).float()\n",
    "\n",
    "\n",
    "A_hat = build_normalized_adj(NUM_CLASSES, edges).to(device)\n",
    "print(\"A_hat built:\", A_hat.shape)\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (2) Label Encoder: GCN\n",
    "# ------------------------------------------------\n",
    "class LabelGCN(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim=256, num_layers=2, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        dims = [in_dim] + [hidden_dim] * num_layers\n",
    "        self.linears = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            self.linears.append(nn.Linear(dims[i], dims[i+1]))\n",
    "\n",
    "    def forward(self, A_hat, H):\n",
    "        x = H  # [C, in_dim]\n",
    "        for i, lin in enumerate(self.linears):\n",
    "            x = A_hat @ x          # GCN aggregation\n",
    "            x = lin(x)\n",
    "            if i < self.num_layers - 1:\n",
    "                x = F.relu(x)\n",
    "                x = self.dropout(x)\n",
    "        return x  # [C, hidden_dim]\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (3) 전체 classifier: TF-IDF doc → projection → dot with label GNN\n",
    "# ------------------------------------------------\n",
    "class TaxonomyClassifier(nn.Module):\n",
    "    def __init__(self, vocab_dim, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        # 문서 임베딩 projection matrix: V → d\n",
    "        self.doc_proj = nn.Linear(vocab_dim, hidden_dim, bias=False)\n",
    "        \n",
    "        # 라벨 GCN\n",
    "        self.label_gcn = LabelGCN(\n",
    "            in_dim=vocab_dim,     # label initial features = TF-IDF class-name vector\n",
    "            hidden_dim=hidden_dim,\n",
    "            num_layers=2,\n",
    "            dropout=0.5\n",
    "        )\n",
    "\n",
    "    def forward(self, doc_feats, label_feats, A_hat):\n",
    "        \"\"\"\n",
    "        doc_feats: [N, V]   TF-IDF 문서 벡터\n",
    "        label_feats: [C, V] TF-IDF 클래스 (name) 벡터\n",
    "        A_hat: [C, C]       taxonomy\n",
    "        \"\"\"\n",
    "        # 1) Document embedding\n",
    "        doc_emb = self.doc_proj(doc_feats)     # [N, d]\n",
    "\n",
    "        # 2) Label embedding via GCN\n",
    "        label_emb = self.label_gcn(A_hat, label_feats)  # [C, d]\n",
    "\n",
    "        # 3) Matching score (bilinear의 단순 버전)\n",
    "        logits = doc_emb @ label_emb.T         # [N, C]\n",
    "\n",
    "        return logits, doc_emb, label_emb\n",
    "\n",
    "\n",
    "print(\"Model definitions loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d716f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# 8. ROUND 1 TRAINING WITH SILVER LABELS v1\n",
    "# ================================================\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (1) Dataset 정의 (sparse TF-IDF → batch마다 dense 변환)\n",
    "# ------------------------------------------------\n",
    "class SilverDataset(Dataset):\n",
    "    def __init__(self, X_csr, y_np, pid_list=None):\n",
    "        \"\"\"\n",
    "        X_csr : scipy.sparse CSR matrix (N, V)\n",
    "        y_np  : numpy array (N, C)  -> silver_labels_v1\n",
    "        pid_list : (옵션) pid 리스트, 나중에 쓸 수도 있음\n",
    "        \"\"\"\n",
    "        self.X = X_csr\n",
    "        self.y = y_np\n",
    "        self.pid_list = pid_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 1) sparse → dense (1, V) → (V,)\n",
    "        x_dense = self.X[idx].toarray().astype(\"float32\").squeeze(0)\n",
    "        y = self.y[idx].astype(\"float32\")\n",
    "        return x_dense, y\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (2) Dataset / DataLoader 생성\n",
    "# ------------------------------------------------\n",
    "train_dataset = SilverDataset(X_train_docs, silver_labels_v1, pid_list_train)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,      # 필요하면 늘려도 됨\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(\"Train dataset size:\", len(train_dataset))\n",
    "print(\"Batch size:\", batch_size)\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (3) 모델 / optimizer / loss 정의\n",
    "# ------------------------------------------------\n",
    "vocab_dim = X_train_docs.shape[1]\n",
    "hidden_dim = 256\n",
    "\n",
    "model = TaxonomyClassifier(vocab_dim=vocab_dim, hidden_dim=hidden_dim).to(device)\n",
    "\n",
    "# Label initial features (GCN 입력용) : class name TF-IDF만 사용\n",
    "label_feats = torch.from_numpy(\n",
    "    X_class_name.toarray().astype(\"float32\")\n",
    ").to(device)   # [C, V]\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (4) Training loop (Round 1)\n",
    "# ------------------------------------------------\n",
    "num_epochs = 2  # 일단 가볍게 1~2 epoch 정도부터 시도\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch_x, batch_y in tqdm(train_loader, desc=f\"Epoch {epoch}\"):\n",
    "        batch_x = batch_x.to(device)        # [B, V]\n",
    "        batch_y = batch_y.to(device)        # [B, C]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits, doc_emb, label_emb = model(batch_x, label_feats, A_hat)  # [B, C]\n",
    "        loss = criterion(logits, batch_y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * batch_x.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(train_dataset)\n",
    "    print(f\"[Epoch {epoch}] avg_loss = {avg_loss:.4f}\")\n",
    "\n",
    "print(\"Round 1 training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655c7443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# 9. SELF-TRAINING: BUILD SILVER LABELS v2 FROM ROUND 1 MODEL\n",
    "# ================================================\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (1) Round 1 prediction 함수 (train 전체 predict)\n",
    "# ------------------------------------------------\n",
    "def predict_train_probs(model, X_csr, label_feats, A_hat, batch_size=64):\n",
    "    \"\"\"\n",
    "    Train 전체 문서에 대해 확률 예측을 반환한다.\n",
    "    return: probs (N_train, NUM_CLASSES)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    N = X_csr.shape[0]\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, N, batch_size), desc=\"Predicting Round 1\"):\n",
    "            X_batch = X_csr[i : i+batch_size].toarray().astype(\"float32\")\n",
    "            X_batch = torch.from_numpy(X_batch).to(device)\n",
    "\n",
    "            logits, _, _ = model(X_batch, label_feats, A_hat)\n",
    "            probs = torch.sigmoid(logits).cpu().numpy()\n",
    "\n",
    "            all_probs.append(probs)\n",
    "\n",
    "    return np.vstack(all_probs)\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (2) Round 1 전체 예측 수행\n",
    "# ------------------------------------------------\n",
    "print(\"Predicting train probabilities (Round 1)...\")\n",
    "\n",
    "probs_train = predict_train_probs(\n",
    "    model,\n",
    "    X_train_docs,\n",
    "    label_feats,\n",
    "    A_hat,\n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "print(\"probs_train shape:\", probs_train.shape)  # (N_train, 531)\n",
    "print(\"probs range:\", probs_train.min(), probs_train.max())\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (3) Silver labels v2 생성\n",
    "#     adaptive threshold:\n",
    "#       HIGH = 0.7  → confident positive\n",
    "#       LOW  = 0.3  → confident negative\n",
    "# ------------------------------------------------\n",
    "def build_silver_labels_v2(probs, silver_v1, high=0.7, low=0.3):\n",
    "    \"\"\"\n",
    "    probs : round 1 predicted probabilities (N, C)\n",
    "    silver_v1 : previous silver labels (N, C)\n",
    "    return: updated silver_v2 (N, C)\n",
    "    \"\"\"\n",
    "    silver_v2 = silver_v1.copy()\n",
    "\n",
    "    pos_mask = probs >= high\n",
    "    neg_mask = probs <= low\n",
    "\n",
    "    silver_v2[pos_mask] = 1.0\n",
    "    silver_v2[neg_mask] = 0.0\n",
    "\n",
    "    return silver_v2\n",
    "\n",
    "\n",
    "silver_labels_v2 = build_silver_labels_v2(\n",
    "    probs_train,\n",
    "    silver_labels_v1,\n",
    "    high=0.7,\n",
    "    low=0.3\n",
    ")\n",
    "\n",
    "print(\"silver_labels_v2 shape:\", silver_labels_v2.shape)\n",
    "print(\"avg positives per doc:\", silver_labels_v2.sum(axis=1).mean())\n",
    "print(\"Self-training Round 2 labels ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07bbb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# 10. ROUND 2 TRAINING WITH SILVER LABELS v2\n",
    "# ================================================\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# SilverDataset 클래스는 이미 위에서 정의해 둔 것을 재사용:\n",
    "# class SilverDataset(Dataset):\n",
    "#     def __init__(self, X_csr, y_np, pid_list=None):\n",
    "#         ...\n",
    "#     def __len__(self):\n",
    "#         ...\n",
    "#     def __getitem__(self, idx):\n",
    "#         ...\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (1) Round 2용 Dataset / DataLoader 생성\n",
    "# ------------------------------------------------\n",
    "train_dataset_v2 = SilverDataset(X_train_docs, silver_labels_v2, pid_list_train)\n",
    "\n",
    "batch_size_round2 = 64\n",
    "train_loader_v2 = DataLoader(\n",
    "    train_dataset_v2,\n",
    "    batch_size=batch_size_round2,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(\"Round 2 train dataset size:\", len(train_dataset_v2))\n",
    "print(\"Round 2 batch size:\", batch_size_round2)\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (2) Optimizer / Loss 재설정 (모델은 Round 1에서 이어서 사용)\n",
    "# ------------------------------------------------\n",
    "criterion_round2 = nn.BCEWithLogitsLoss()\n",
    "optimizer_round2 = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (3) Round 2 Training Loop\n",
    "# ------------------------------------------------\n",
    "num_epochs_round2 = 2  # Round 2에서도 1~2 epoch 정도 돌려보는 것을 추천\n",
    "\n",
    "for epoch in range(1, num_epochs_round2 + 1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch_x, batch_y in tqdm(train_loader_v2, desc=f\"[Round 2] Epoch {epoch}\"):\n",
    "        batch_x = batch_x.to(device)   # [B, V]\n",
    "        batch_y = batch_y.to(device)   # [B, C]\n",
    "\n",
    "        optimizer_round2.zero_grad()\n",
    "\n",
    "        # label_feats, A_hat, model 은 Round 1에서 이미 정의된 것을 사용\n",
    "        logits, doc_emb, label_emb = model(batch_x, label_feats, A_hat)  # [B, C]\n",
    "        loss = criterion_round2(logits, batch_y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer_round2.step()\n",
    "\n",
    "        total_loss += loss.item() * batch_x.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(train_dataset_v2)\n",
    "    print(f\"[Round 2 - Epoch {epoch}] avg_loss = {avg_loss:.4f}\")\n",
    "\n",
    "print(\"Round 2 training finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806fe56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# 11. TEST PREDICTION + SUBMISSION FILE\n",
    "# ================================================\n",
    "\n",
    "def predict_test_probs(model, X_csr, label_feats, A_hat, batch_size=64):\n",
    "    \"\"\"\n",
    "    Test 전체 문서에 대해 확률 예측을 반환한다.\n",
    "    return: probs (N_test, NUM_CLASSES)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    N = X_csr.shape[0]\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, N, batch_size), desc=\"Predicting Test Set\"):\n",
    "            X_batch = X_csr[i : i+batch_size].toarray().astype(\"float32\")\n",
    "            X_batch = torch.from_numpy(X_batch).to(device)\n",
    "\n",
    "            logits, _, _ = model(X_batch, label_feats, A_hat)\n",
    "            probs = torch.sigmoid(logits).cpu().numpy()\n",
    "\n",
    "            all_probs.append(probs)\n",
    "\n",
    "    return np.vstack(all_probs)\n",
    "\n",
    "\n",
    "print(\"Predicting on test corpus...\")\n",
    "probs_test = predict_test_probs(\n",
    "    model,\n",
    "    X_test_docs,\n",
    "    label_feats,\n",
    "    A_hat,\n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "print(\"probs_test shape:\", probs_test.shape)  # (N_test, 531)\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (2) 문서당 Top-K label 선택\n",
    "# ------------------------------------------------\n",
    "\n",
<<<<<<< HEAD
    "MIN_LABELS = 2\n",
=======
    "MIN_LABELS = 1\n",
>>>>>>> abbf8fff3daf2b0d2a1bd36649e418d6fb03b845
    "MAX_LABELS = 3\n",
    "\n",
    "def pick_labels(prob_row, min_k=1, max_k=3):\n",
    "    \"\"\"\n",
    "    한 문서의 확률벡터에서 top-K 라벨 선택.\n",
    "    \"\"\"\n",
    "    # 확률이 높은 class 순으로 정렬\n",
    "    sorted_idx = np.argsort(prob_row)[::-1]  # 내림차순\n",
    "    topk = sorted_idx[:max_k]\n",
    "    return list(topk)\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (3) submission.csv 파일 생성\n",
    "# ------------------------------------------------\n",
    "print(\"Generating submission.csv...\")\n",
    "\n",
    "with open(SUBMISSION_PATH, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"pid\", \"labels\"])  # header\n",
    "\n",
    "    for i, pid in enumerate(pid_list_test):\n",
    "        labels = pick_labels(probs_test[i], MIN_LABELS, MAX_LABELS)\n",
    "        label_str = \",\".join(map(str, labels))\n",
    "        writer.writerow([pid, label_str])\n",
    "\n",
    "print(\"Submission file saved to:\", SUBMISSION_PATH)\n",
    "print(\"Total test samples:\", len(pid_list_test))\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ce8f55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063eab09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c062713b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886663cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084431fe-97fd-442e-9bb7-78f1153f80c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b486e5-1fb7-4e9e-b63a-bc3919ab81ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e97ef5e-5ee1-4bb1-9ccd-d0242241c2b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312fa965-ac9f-4255-a3d7-3466aba485f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7419c2a8-6739-4ed6-b9ee-3eeb108873de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating dummy predictions: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 19658/19658 [00:00<00:00, 190266.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy submission file saved to: submission.csv\n",
      "Total samples: 19658, Classes per sample: 1-3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ------------------------\n",
    "# Dummy baseline for Kaggle submission\n",
    "# Generates random multi-label predictions\n",
    "# ------------------------\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Paths ---\n",
    "TEST_DIR = \"Amazon_products/test\"  # modify if needed\n",
    "TEST_CORPUS_PATH = os.path.join(TEST_DIR, \"test_corpus.txt\")  # product_id \\t text\n",
    "SUBMISSION_PATH = \"submission.csv\"  # output file\n",
    "\n",
    "# --- Constants ---\n",
    "NUM_CLASSES = 531  # total number of classes (0–530)\n",
    "MIN_LABELS = 1     # minimum number of labels per sample\n",
    "MAX_LABELS = 3     # maximum number of labels per sample\n",
    "\n",
    "# --- Load test corpus ---\n",
    "def load_corpus(path):\n",
    "    \"\"\"Load test corpus into {pid: text} dictionary.\"\"\"\n",
    "    pid2text = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\"\\t\", 1)\n",
    "            if len(parts) == 2:\n",
    "                pid, text = parts\n",
    "                pid2text[pid] = text\n",
    "    return pid2text\n",
    "\n",
    "pid2text_test = load_corpus(TEST_CORPUS_PATH)\n",
    "pid_list_test = list(pid2text_test.keys())\n",
    "\n",
    "# --- Generate random predictions ---\n",
    "all_pids, all_labels = [], []\n",
    "for pid in tqdm(pid_list_test, desc=\"Generating dummy predictions\"):\n",
    "    n_labels = random.randint(MIN_LABELS, MAX_LABELS)\n",
    "    labels = random.sample(range(NUM_CLASSES), n_labels)\n",
    "    labels = sorted(labels)\n",
    "    all_pids.append(pid)\n",
    "    all_labels.append(labels)\n",
    "\n",
    "# --- Save submission file ---\n",
    "with open(SUBMISSION_PATH, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"pid\", \"labels\"])\n",
    "    for pid, labels in zip(all_pids, all_labels):\n",
    "        writer.writerow([pid, \",\".join(map(str, labels))])\n",
    "\n",
    "print(f\"Dummy submission file saved to: {SUBMISSION_PATH}\")\n",
    "print(f\"Total samples: {len(all_pids)}, Classes per sample: {MIN_LABELS}-{MAX_LABELS}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "23csp",
   "language": "python",
   "name": "23csp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
