{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34537235",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T10:03:34.062965Z",
     "iopub.status.busy": "2025-11-16T10:03:34.062655Z",
     "iopub.status.idle": "2025-11-16T10:03:35.603891Z",
     "shell.execute_reply": "2025-11-16T10:03:35.603286Z",
     "shell.execute_reply.started": "2025-11-16T10:03:34.062946Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "\n",
      "== Data path check ==\n",
      "Amazon_products/train/train_corpus.txt -> True\n",
      "Amazon_products/test/test_corpus.txt -> True\n",
      "Amazon_products/classes.txt -> True\n",
      "Amazon_products/class_hierarchy.txt -> True\n",
      "Amazon_products/class_related_keywords.txt -> True\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# 1. REPRODUCIBILITY SETTINGS \n",
    "# ================================================\n",
    "from pathlib import Path\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "# ----- Reproducibility -----\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# ----- Device -----\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# ================================================\n",
    "# 2. DEFAULT PATHS FOR DATASET\n",
    "# ================================================\n",
    "ROOT = Path(\"Amazon_products\")   # dataset root directory\n",
    "\n",
    "# Main corpus\n",
    "TRAIN_CORPUS_PATH = ROOT / \"train\" /  \"train_corpus.txt\"       # pid \\t text\n",
    "TEST_CORPUS_PATH  = ROOT / \"test\" / \"test_corpus.txt\"        # pid \\t text\n",
    "\n",
    "# Taxonomy & class meta\n",
    "CLASSES_PATH      = ROOT / \"classes.txt\"            # class_id \\t class_name\n",
    "HIERARCHY_PATH    = ROOT / \"class_hierarchy.txt\"    # parent_id \\t child_id\n",
    "KEYWORDS_PATH     = ROOT / \"class_related_keywords.txt\"\n",
    "\n",
    "# Constants\n",
    "NUM_CLASSES = 531\n",
    "MIN_LABELS = 1     # minimum number of labels per sample\n",
    "MAX_LABELS = 3     # maximum number of labels per sample\n",
    "\n",
    "# Check paths\n",
    "print(\"\\n== Data path check ==\")\n",
    "for p in [TRAIN_CORPUS_PATH, TEST_CORPUS_PATH,\n",
    "          CLASSES_PATH, HIERARCHY_PATH, KEYWORDS_PATH]:\n",
    "    print(f\"{p} -> {p.exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95583a7e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T10:03:35.604867Z",
     "iopub.status.busy": "2025-11-16T10:03:35.604634Z",
     "iopub.status.idle": "2025-11-16T10:03:35.668075Z",
     "shell.execute_reply": "2025-11-16T10:03:35.667556Z",
     "shell.execute_reply.started": "2025-11-16T10:03:35.604849Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train/test corpus...\n",
      "Train samples: 29487\n",
      "Test samples : 19658\n",
      "Example train sample #0: pid=0, text=omron hem 790it automatic blood pressure monitor with advanced omron health mana...\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# 2. DATA LOADING\n",
    "# ================================================\n",
    "\n",
    "def load_corpus(path):\n",
    "    \"\"\"\n",
    "    Load corpus file (pid \\\\t text) as {pid: text} dictionary.\n",
    "    \"\"\"\n",
    "    pid2text = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\"\\t\", 1)\n",
    "            if len(parts) == 2:\n",
    "                pid, text = parts\n",
    "                pid2text[pid] = text\n",
    "    return pid2text\n",
    "\n",
    "# Load corpus\n",
    "print(\"Loading train/test corpus...\")\n",
    "\n",
    "pid2text_train = load_corpus(TRAIN_CORPUS_PATH)\n",
    "pid2text_test  = load_corpus(TEST_CORPUS_PATH)\n",
    "\n",
    "pid_list_train = list(pid2text_train.keys())\n",
    "pid_list_test  = list(pid2text_test.keys())\n",
    "\n",
    "print(\"Train samples:\", len(pid2text_train))\n",
    "print(\"Test samples :\", len(pid2text_test))\n",
    "\n",
    "# Quick sample check\n",
    "for i, (pid, text) in enumerate(pid2text_train.items()):\n",
    "    print(f\"Example train sample #{i}: pid={pid}, text={text[:80]}...\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "968916cf-d39c-4884-a813-7daeed03f98e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T10:03:35.668724Z",
     "iopub.status.busy": "2025-11-16T10:03:35.668561Z",
     "iopub.status.idle": "2025-11-16T10:03:35.678202Z",
     "shell.execute_reply": "2025-11-16T10:03:35.677717Z",
     "shell.execute_reply.started": "2025-11-16T10:03:35.668707Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading class metadata...\n",
      "Num classes: 531\n",
      "Num edges in taxonomy: 568\n",
      "\n",
      "Example class id: 0\n",
      "Name: grocery_gourmet_food\n",
      "Keywords: ['snacks', 'condiments', 'beverages', 'specialty_foods', 'spices', 'cooking_oils', 'baking_ingredients', 'gourmet_chocolates', 'artisanal_cheeses', 'organic_foods']\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# 3. CLASS METADATA LOADING\n",
    "# ================================================\n",
    "\n",
    "def load_classes(path):\n",
    "    \"\"\"\n",
    "    classes.txt : class_id \\\\t class_name\n",
    "    returns: id2label, label2id\n",
    "    \"\"\"\n",
    "    id2label = {}\n",
    "    label2id = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\"\\t\")\n",
    "            if len(parts) != 2:\n",
    "                continue\n",
    "            cid, name = parts\n",
    "            cid = int(cid)\n",
    "            id2label[cid] = name\n",
    "            label2id[name] = cid\n",
    "    return id2label, label2id\n",
    "\n",
    "\n",
    "def load_hierarchy(path):\n",
    "    \"\"\"\n",
    "    class_hierarchy.txt : parent_id \\\\t child_id\n",
    "    returns: edges (list of tuples)\n",
    "    \"\"\"\n",
    "    edges = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\"\\t\")\n",
    "            if len(parts) != 2:\n",
    "                continue\n",
    "            p, c = map(int, parts)\n",
    "            edges.append((p, c))\n",
    "    return edges\n",
    "\n",
    "\n",
    "def load_keywords(path, label2id):\n",
    "    \"\"\"\n",
    "    class_related_keywords.txt : CLASS_NAME: kw1, kw2,...\n",
    "    returns: {class_id: [kws]}\n",
    "    \"\"\"\n",
    "    d = {cid: [] for cid in label2id.values()}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if \":\" not in line:\n",
    "                continue\n",
    "            name, kws = line.strip().split(\":\", 1)\n",
    "            kws = [k.strip() for k in kws.split(\",\") if k.strip()]\n",
    "            if name in label2id:\n",
    "                cid = label2id[name]\n",
    "                d[cid] = kws\n",
    "    return d\n",
    "\n",
    "\n",
    "# ----------------- Load all class meta -----------------\n",
    "print(\"Loading class metadata...\")\n",
    "\n",
    "id2label, label2id = load_classes(CLASSES_PATH)\n",
    "edges = load_hierarchy(HIERARCHY_PATH)\n",
    "label_keywords = load_keywords(KEYWORDS_PATH, label2id)\n",
    "\n",
    "print(\"Num classes:\", len(id2label))\n",
    "print(\"Num edges in taxonomy:\", len(edges))\n",
    "print()\n",
    "\n",
    "# Small check\n",
    "example_id = 0\n",
    "print(\"Example class id:\", example_id)\n",
    "print(\"Name:\", id2label[example_id])\n",
    "print(\"Keywords:\", label_keywords[example_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19b383df-383e-4b23-949b-1190afd9564e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T10:03:35.679183Z",
     "iopub.status.busy": "2025-11-16T10:03:35.679023Z",
     "iopub.status.idle": "2025-11-16T10:03:46.415009Z",
     "shell.execute_reply": "2025-11-16T10:03:46.414434Z",
     "shell.execute_reply.started": "2025-11-16T10:03:35.679168Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example class name text: grocery gourmet food\n",
      "Example class keyword text: snacks condiments beverages specialty_foods spices cooking_oils baking_ingredients gourmet_chocolates artisanal_cheeses organic_foods snacks condiments beverages specialty_foods spices cooking_oils baking_ingredients gourmet_chocolates artisanal_cheeses organic_foods snacks condiments beverages specialty_foods spices cooking_oils baking_ingredients gourmet_chocolates artisanal_cheeses organic_foods\n",
      "Vocabulary size: 100000\n",
      "X_train_docs: (29487, 100000)\n",
      "X_test_docs : (19658, 100000)\n",
      "X_class_name: (531, 100000)\n",
      "X_class_kw  : (531, 100000)\n",
      "\n",
      "Normalizing TF-IDF vectors...\n",
      "\n",
      "Computing doc–class cosine similarities...\n",
      "sims_name shape: (29487, 531) | min/max: 0.0 0.5831636\n",
      "sims_kw   shape: (29487, 531) | min/max: 0.0 0.40836385\n",
      "\n",
      "Final sims shape: (29487, 531)\n",
      "Final sims min/max: 0.0 0.2858547\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# 4. TF-IDF EMBEDDING + DOC–CLASS SIMILARITY\n",
    "#    (class name vs keyword 분리 활용)\n",
    "# ================================================\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (1) 클래스 이름 텍스트 / 키워드 텍스트 분리 생성\n",
    "# ------------------------------------------------\n",
    "def build_class_name_texts(id2label):\n",
    "    \"\"\"\n",
    "    각 class_id에 대해 클래스 이름만 사용한 텍스트 리스트 생성\n",
    "    \"\"\"\n",
    "    texts = []\n",
    "    for cid in range(NUM_CLASSES):\n",
    "        name = id2label[cid].replace(\"_\", \" \")\n",
    "        texts.append(name)\n",
    "    return texts\n",
    "\n",
    "def build_class_keyword_texts(label_keywords, repeat=3):\n",
    "    texts = []\n",
    "    for cid in range(NUM_CLASSES):\n",
    "        kws_list = label_keywords.get(cid, [])\n",
    "        if not kws_list:\n",
    "            texts.append(\"\")\n",
    "            continue\n",
    "        kws = \" \".join(kws_list)\n",
    "        # 키워드 텍스트를 반복해서 TF-IDF 상에서 weight 증가\n",
    "        text = (\" \" + kws) * repeat\n",
    "        texts.append(text.strip())\n",
    "    return texts\n",
    "\n",
    "\n",
    "class_name_texts = build_class_name_texts(id2label)        \n",
    "class_kw_texts = build_class_keyword_texts(label_keywords, repeat=3)    \n",
    "\n",
    "print(\"Example class name text:\", class_name_texts[0])\n",
    "print(\"Example class keyword text:\", class_kw_texts[0])\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (2) TF-IDF vocabulary 학습\n",
    "#     - train 문서 + test 문서 + class name + class keyword 모두 포함\n",
    "# ------------------------------------------------\n",
    "all_texts_for_vocab = (\n",
    "    list(pid2text_train.values())\n",
    "    + list(pid2text_test.values())\n",
    "    + class_name_texts\n",
    "    + class_kw_texts\n",
    ")\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=100_000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=1,\n",
    "    sublinear_tf=True\n",
    ")\n",
    "\n",
    "vectorizer.fit(all_texts_for_vocab)\n",
    "\n",
    "print(\"Vocabulary size:\", len(vectorizer.vocabulary_))\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (3) 실제 TF-IDF 행렬 변환\n",
    "# ------------------------------------------------\n",
    "N_train = len(pid2text_train)\n",
    "N_test  = len(pid2text_test)\n",
    "C       = NUM_CLASSES\n",
    "\n",
    "# 문서 TF-IDF\n",
    "X_train_docs = vectorizer.transform(pid2text_train.values())   # [N_train, V]\n",
    "X_test_docs  = vectorizer.transform(pid2text_test.values())    # [N_test, V]\n",
    "\n",
    "# 클래스 이름 TF-IDF (GNN용 initial feature)\n",
    "X_class_name = vectorizer.transform(class_name_texts)          # [C, V]\n",
    "\n",
    "# 클래스 키워드 TF-IDF (silver label용 보조 sim)\n",
    "X_class_kw   = vectorizer.transform(class_kw_texts)            # [C, V]\n",
    "\n",
    "print(\"X_train_docs:\", X_train_docs.shape)\n",
    "print(\"X_test_docs :\", X_test_docs.shape)\n",
    "print(\"X_class_name:\", X_class_name.shape)\n",
    "print(\"X_class_kw  :\", X_class_kw.shape)\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (4) L2 정규화 (코사인 유사도용)\n",
    "# ------------------------------------------------\n",
    "print(\"\\nNormalizing TF-IDF vectors...\")\n",
    "X_train_norm      = normalize(X_train_docs, axis=1)\n",
    "X_class_name_norm = normalize(X_class_name, axis=1)\n",
    "X_class_kw_norm   = normalize(X_class_kw, axis=1)\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (5) 이름 기반 / 키워드 기반 doc–class 유사도 계산\n",
    "# ------------------------------------------------\n",
    "print(\"\\nComputing doc–class cosine similarities...\")\n",
    "\n",
    "# 이름 기반 similarity\n",
    "sims_name = (X_train_norm @ X_class_name_norm.T).toarray().astype(\"float32\")  # [N_train, C]\n",
    "\n",
    "# 키워드 기반 similarity\n",
    "sims_kw   = (X_train_norm @ X_class_kw_norm.T).toarray().astype(\"float32\")    # [N_train, C]\n",
    "\n",
    "print(\"sims_name shape:\", sims_name.shape, \"| min/max:\", sims_name.min(), sims_name.max())\n",
    "print(\"sims_kw   shape:\", sims_kw.shape,   \"| min/max:\", sims_kw.min(),   sims_kw.max())\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (6) 두 채널을 가중합하여 최종 similarity 생성\n",
    "#     - alpha: class name 비중\n",
    "#     - beta : keyword 비중\n",
    "# ------------------------------------------------\n",
    "alpha = 0.3  # class name 중요도\n",
    "beta  = 0.7  # keyword 중요도\n",
    "\n",
    "sims = alpha * sims_name + beta * sims_kw   # [N_train, C]\n",
    "\n",
    "print(\"\\nFinal sims shape:\", sims.shape)\n",
    "print(\"Final sims min/max:\", sims.min(), sims.max())\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "330b238d-d0cb-4e94-a9d0-4e4eab9ed0fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T10:03:46.415742Z",
     "iopub.status.busy": "2025-11-16T10:03:46.415558Z",
     "iopub.status.idle": "2025-11-16T10:03:46.422378Z",
     "shell.execute_reply": "2025-11-16T10:03:46.421861Z",
     "shell.execute_reply.started": "2025-11-16T10:03:46.415723Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-Down exploration module loaded.\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# 5. TOP-DOWN CLASS EXPLORATION (TaxoClass style)\n",
    "# ================================================\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (1) adjacency (parent2child, child2parent) 만들기\n",
    "# ------------------------------------------------\n",
    "parent2child = defaultdict(list)\n",
    "child2parent = defaultdict(list)\n",
    "\n",
    "for p, c in edges:   # edges = [(parent, child), ...]\n",
    "    parent2child[p].append(c)\n",
    "    child2parent[c].append(p)\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (2) Top-Down 탐색 함수\n",
    "# ------------------------------------------------\n",
    "def topdown_candidates(doc_idx, sims_row, root=0, max_depth=5, base_k=2):\n",
    "    \"\"\"\n",
    "    문서-클래스 유사도 sims_row 를 이용하여\n",
    "    taxonomy 기반 top-down class 후보를 탐색.\n",
    "\n",
    "    Return:\n",
    "        candidates (set): 후보 class IDs\n",
    "        path_score (dict): 각 class의 path score\n",
    "    \"\"\"\n",
    "    path_score = {root: 1.0}   # Root의 path score = 1\n",
    "    level_nodes = [root]\n",
    "    visited = set([root])\n",
    "\n",
    "    for depth in range(max_depth):\n",
    "        # 현재 level의 child 후보 모으기\n",
    "        cand = []\n",
    "        for node in level_nodes:\n",
    "            for ch in parent2child.get(node, []):\n",
    "                if ch not in visited:\n",
    "                    cand.append(ch)\n",
    "\n",
    "        # 더 이상 확장할 child가 없으면 종료\n",
    "        if not cand:\n",
    "            break\n",
    "\n",
    "        # child path score 계산\n",
    "        for ch in cand:\n",
    "            parents = child2parent.get(ch, [])\n",
    "            if not parents:\n",
    "                continue\n",
    "            # parent 중 path_score*sim 이 가장 높은 parent 선택\n",
    "            ps = max(\n",
    "                path_score[p] * sims_row[ch]\n",
    "                for p in parents\n",
    "                if p in path_score\n",
    "            )\n",
    "            path_score[ch] = ps\n",
    "\n",
    "        # path_score 기준 상위 k개 선택\n",
    "        k = base_k + depth    # depth=0→2, depth=1→3, ...\n",
    "        cand_sorted = sorted(\n",
    "            cand,\n",
    "            key=lambda x: path_score.get(x, 0.0),\n",
    "            reverse=True\n",
    "        )\n",
    "        level_nodes = cand_sorted[:k]\n",
    "\n",
    "        visited.update(level_nodes)\n",
    "\n",
    "    # root 제외\n",
    "    candidates = visited - {root}\n",
    "    return candidates, path_score\n",
    "\n",
    "\n",
    "print(\"Top-Down exploration module loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13f8ca6b-a496-43b8-880e-8e2b6589ff3b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T10:03:46.422915Z",
     "iopub.status.busy": "2025-11-16T10:03:46.422757Z",
     "iopub.status.idle": "2025-11-16T10:03:48.802115Z",
     "shell.execute_reply": "2025-11-16T10:03:48.801605Z",
     "shell.execute_reply.started": "2025-11-16T10:03:46.422898Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating silver_labels_v1 (with mask) using sims...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building silver labels v1 (with mask): 100%|██████████| 29487/29487 [00:02<00:00, 12504.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "silver_labels_v1 shape: (29487, 531)\n",
      "silver_mask_v1   shape: (29487, 531)\n",
      "avg positives per doc: 2.1752975\n",
      "avg supervised classes per doc (|Q_i|): 6.76939\n",
      "docs with 0 supervised classes: 0\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# 6. CORE CLASS MINING + SILVER LABEL v1 (WITH MASK)\n",
    "# ================================================\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---------------------------\n",
    "# (1) core class 함수 (margin 완화 + fallback 포함)\n",
    "# ---------------------------\n",
    "def get_core_classes_for_doc(sims_row, candidates, top_m=3, margin=0.0):\n",
    "    \"\"\"\n",
    "    sims_row : shape (NUM_CLASSES,)\n",
    "    candidates : topdown_candidates()로 얻은 후보 class 집합\n",
    "    top_m : core class 최대 개수\n",
    "    margin : parent/sibling보다 얼마나 더 커야 core로 인정할지\n",
    "    \"\"\"\n",
    "    confs = []  # (cid, conf)\n",
    "\n",
    "    for c in candidates:\n",
    "        parents = child2parent.get(c, [])\n",
    "        sibs = set()\n",
    "        for p in parents:\n",
    "            sibs.update(parent2child.get(p, []))\n",
    "        sibs.discard(c)\n",
    "\n",
    "        base_sim = 0.0\n",
    "        idxs = list(parents) + list(sibs)\n",
    "        if idxs:\n",
    "            base_sim = sims_row[idxs].max()\n",
    "\n",
    "        conf = float(sims_row[c] - base_sim)\n",
    "        confs.append((c, conf))\n",
    "\n",
    "    # margin 이상인 것만\n",
    "    good = [(c, conf) for c, conf in confs if conf > margin]\n",
    "    good.sort(key=lambda x: x[1], reverse=True)\n",
    "    core = [c for c, _ in good[:top_m]]\n",
    "\n",
    "    # fallback: core가 비면 sims_row 기준 top-1 후보를 core로\n",
    "    if not core and len(candidates) > 0:\n",
    "        best_c = max(candidates, key=lambda cid: sims_row[cid])\n",
    "        core = [best_c]\n",
    "\n",
    "    return core\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# (2) silver v1 + mask 생성\n",
    "# ---------------------------\n",
    "def build_silver_labels_v1_with_mask(\n",
    "    sims,\n",
    "    max_depth=5,\n",
    "    base_k=2,\n",
    "    top_m=3,\n",
    "    margin=0.0\n",
    "):\n",
    "    \"\"\"\n",
    "    sims: (N_train, NUM_CLASSES)\n",
    "    return:\n",
    "        silver_y : (N, C) float32  (0/1)\n",
    "        silver_m : (N, C) float32  (0/1, mask)\n",
    "    - 문서별 후보 집합 Q_i 안에서만 0/1 라벨을 정의\n",
    "    - Q_i 밖은 mask=0 → loss에서 제외\n",
    "    \"\"\"\n",
    "    N_train = sims.shape[0]\n",
    "    silver_y = np.zeros((N_train, NUM_CLASSES), dtype=np.float32)\n",
    "    silver_m = np.zeros((N_train, NUM_CLASSES), dtype=np.float32)\n",
    "\n",
    "    for i in tqdm(range(N_train), desc=\"Building silver labels v1 (with mask)\"):\n",
    "        sims_row = sims[i]\n",
    "\n",
    "        # 1) Top-Down candidates\n",
    "        candidates, _ = topdown_candidates(\n",
    "            doc_idx=i,\n",
    "            sims_row=sims_row,\n",
    "            root=0,\n",
    "            max_depth=max_depth,\n",
    "            base_k=base_k,\n",
    "        )\n",
    "\n",
    "        # 후보가 없다면 sims top_m을 후보로 사용\n",
    "        if not candidates:\n",
    "            topk = np.argsort(sims_row)[::-1][:top_m]\n",
    "            candidates = set(topk)\n",
    "\n",
    "        # 2) core class 선택\n",
    "        cores = get_core_classes_for_doc(\n",
    "            sims_row,\n",
    "            candidates,\n",
    "            top_m=top_m,\n",
    "            margin=margin,\n",
    "        )\n",
    "\n",
    "        # 그래도 core가 없다면 sims top-1을 core로\n",
    "        if not cores:\n",
    "            best_c = int(np.argmax(sims_row))\n",
    "            cores = [best_c]\n",
    "\n",
    "        # 3) positive set = core + parents(core)\n",
    "        pos = set()\n",
    "        for c in cores:\n",
    "            pos.add(c)\n",
    "            for p in child2parent.get(c, []):\n",
    "                pos.add(p)\n",
    "\n",
    "        # 4) candidate set Q_i = candidates + parents(candidates)\n",
    "        Q = set(candidates)\n",
    "        for c in list(candidates):\n",
    "            for p in child2parent.get(c, []):\n",
    "                Q.add(p)\n",
    "\n",
    "        # pos는 반드시 Q 안에 있어야 함\n",
    "        Q.update(pos)\n",
    "\n",
    "        # 5) Q_i 안에서만 0/1 라벨 정의\n",
    "        for c in Q:\n",
    "            silver_m[i, c] = 1.0              # 이 클래스는 supervision 대상\n",
    "            silver_y[i, c] = 1.0 if c in pos else 0.0\n",
    "\n",
    "    return silver_y, silver_m\n",
    "\n",
    "\n",
    "print(\"Generating silver_labels_v1 (with mask) using sims...\")\n",
    "\n",
    "silver_labels_v1, silver_mask_v1 = build_silver_labels_v1_with_mask(\n",
    "    sims,\n",
    "    max_depth=5,\n",
    "    base_k=2,\n",
    "    top_m=3,\n",
    "    margin=0.0,   # 일단 margin=0으로 두고, 나중에 조정 가능\n",
    ")\n",
    "\n",
    "print(\"silver_labels_v1 shape:\", silver_labels_v1.shape)\n",
    "print(\"silver_mask_v1   shape:\", silver_mask_v1.shape)\n",
    "\n",
    "pos_per_doc = silver_labels_v1.sum(axis=1)\n",
    "sup_per_doc = silver_mask_v1.sum(axis=1)\n",
    "print(\"avg positives per doc:\", pos_per_doc.mean())\n",
    "print(\"avg supervised classes per doc (|Q_i|):\", sup_per_doc.mean())\n",
    "print(\"docs with 0 supervised classes:\", (sup_per_doc == 0).sum())\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "569ac2ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T10:03:48.802751Z",
     "iopub.status.busy": "2025-11-16T10:03:48.802583Z",
     "iopub.status.idle": "2025-11-16T10:03:48.965327Z",
     "shell.execute_reply": "2025-11-16T10:03:48.964692Z",
     "shell.execute_reply.started": "2025-11-16T10:03:48.802733Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_hat built: torch.Size([531, 531])\n",
      "Model definitions loaded.\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# 7. LABEL-GCN + DOCUMENT-CLASS CLASSIFIER\n",
    "# ================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (1) Build normalized adjacency A_hat for GCN\n",
    "# ------------------------------------------------\n",
    "def build_normalized_adj(num_classes, edges):\n",
    "    \"\"\"\n",
    "    edges: [(parent, child), ...]\n",
    "    출력: A_hat (torch.FloatTensor, [C,C])\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    A = np.zeros((num_classes, num_classes), dtype=np.float32)\n",
    "\n",
    "    # parent-child 연결을 양방향으로 넣기\n",
    "    for p, c in edges:\n",
    "        A[p, c] = 1.0\n",
    "        A[c, p] = 1.0\n",
    "\n",
    "    # self-loop\n",
    "    np.fill_diagonal(A, 1.0)\n",
    "\n",
    "    # D^{-1/2} * A * D^{-1/2}\n",
    "    deg = A.sum(axis=1)\n",
    "    deg_inv_sqrt = np.power(deg, -0.5)\n",
    "    deg_inv_sqrt[np.isinf(deg_inv_sqrt)] = 0.0\n",
    "    D_inv_sqrt = np.diag(deg_inv_sqrt)\n",
    "\n",
    "    A_hat = D_inv_sqrt @ A @ D_inv_sqrt\n",
    "    return torch.from_numpy(A_hat).float()\n",
    "\n",
    "\n",
    "A_hat = build_normalized_adj(NUM_CLASSES, edges).to(device)\n",
    "print(\"A_hat built:\", A_hat.shape)\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (2) Label Encoder: GCN\n",
    "# ------------------------------------------------\n",
    "class LabelGCN(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim=256, num_layers=2, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        dims = [in_dim] + [hidden_dim] * num_layers\n",
    "        self.linears = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            self.linears.append(nn.Linear(dims[i], dims[i+1]))\n",
    "\n",
    "    def forward(self, A_hat, H):\n",
    "        x = H  # [C, in_dim]\n",
    "        for i, lin in enumerate(self.linears):\n",
    "            x = A_hat @ x          # GCN aggregation\n",
    "            x = lin(x)\n",
    "            if i < self.num_layers - 1:\n",
    "                x = F.relu(x)\n",
    "                x = self.dropout(x)\n",
    "        return x  # [C, hidden_dim]\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (3) 전체 classifier: TF-IDF doc → projection → dot with label GNN\n",
    "# ------------------------------------------------\n",
    "class TaxonomyClassifier(nn.Module):\n",
    "    def __init__(self, vocab_dim, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        # 문서 임베딩 projection matrix: V → d\n",
    "        self.doc_proj = nn.Linear(vocab_dim, hidden_dim, bias=False)\n",
    "        \n",
    "        # 라벨 GCN\n",
    "        self.label_gcn = LabelGCN(\n",
    "            in_dim=vocab_dim,     # label initial features = TF-IDF class-name vector\n",
    "            hidden_dim=hidden_dim,\n",
    "            num_layers=2,\n",
    "            dropout=0.5\n",
    "        )\n",
    "\n",
    "    def forward(self, doc_feats, label_feats, A_hat):\n",
    "        \"\"\"\n",
    "        doc_feats: [N, V]   TF-IDF 문서 벡터\n",
    "        label_feats: [C, V] TF-IDF 클래스 (name) 벡터\n",
    "        A_hat: [C, C]       taxonomy\n",
    "        \"\"\"\n",
    "        # 1) Document embedding\n",
    "        doc_emb = self.doc_proj(doc_feats)     # [N, d]\n",
    "\n",
    "        # 2) Label embedding via GCN\n",
    "        label_emb = self.label_gcn(A_hat, label_feats)  # [C, d]\n",
    "\n",
    "        # 3) Matching score (bilinear의 단순 버전)\n",
    "        logits = doc_emb @ label_emb.T         # [N, C]\n",
    "\n",
    "        return logits, doc_emb, label_emb\n",
    "\n",
    "\n",
    "print(\"Model definitions loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51aa9273-a91b-410e-bfbf-f3643fd1baf0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T10:03:48.966063Z",
     "iopub.status.busy": "2025-11-16T10:03:48.965886Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 29487\n",
      "Batch size: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Round 1] Epoch 1: 100%|█████████▉| 459/461 [00:17<00:00, 26.43it/s]"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# 8. ROUND 1 TRAINING (MASKED BCE WITH SILVER v1)\n",
    "# ================================================\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SilverDatasetWithMask(Dataset):\n",
    "    def __init__(self, X_csr, y_np, m_np, pid_list=None):\n",
    "        self.X = X_csr\n",
    "        self.y = y_np\n",
    "        self.m = m_np\n",
    "        self.pid_list = pid_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x_dense = self.X[idx].toarray().astype(\"float32\").squeeze(0)\n",
    "        y = self.y[idx].astype(\"float32\")\n",
    "        m = self.m[idx].astype(\"float32\")\n",
    "        return x_dense, y, m\n",
    "\n",
    "\n",
    "train_dataset = SilverDatasetWithMask(X_train_docs, silver_labels_v1, silver_mask_v1, pid_list_train)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(\"Train dataset size:\", len(train_dataset))\n",
    "print(\"Batch size:\", batch_size)\n",
    "\n",
    "vocab_dim = X_train_docs.shape[1]\n",
    "hidden_dim = 256\n",
    "\n",
    "model = TaxonomyClassifier(vocab_dim=vocab_dim, hidden_dim=hidden_dim).to(device)\n",
    "\n",
    "label_feats = torch.from_numpy(\n",
    "    X_class_name.toarray().astype(\"float32\")\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(reduction='none')  # <-- 중요: reduction='none'\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 2\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_count = 0.0   # 실제 supervision에 사용된 (doc, class) 개수\n",
    "\n",
    "    for batch_x, batch_y, batch_m in tqdm(train_loader, desc=f\"[Round 1] Epoch {epoch}\"):\n",
    "        batch_x = batch_x.to(device)    # [B, V]\n",
    "        batch_y = batch_y.to(device)    # [B, C]\n",
    "        batch_m = batch_m.to(device)    # [B, C]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits, doc_emb, label_emb = model(batch_x, label_feats, A_hat)  # [B, C]\n",
    "        loss_raw = criterion(logits, batch_y)  # [B, C]\n",
    "\n",
    "        # mask 적용: Q_i 안에서만 loss 계산\n",
    "        loss_masked = loss_raw * batch_m\n",
    "        loss = loss_masked.sum() / (batch_m.sum() + 1e-8)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * batch_m.sum().item()\n",
    "        total_count += batch_m.sum().item()\n",
    "\n",
    "    avg_loss = total_loss / (total_count + 1e-8)\n",
    "    print(f\"[Round 1] Epoch {epoch} avg_loss = {avg_loss:.6f}\")\n",
    "\n",
    "print(\"Round 1 training (masked BCE) finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256560ad-331c-456c-8fc5-92ac650c9e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# 9. SELF-TRAINING: SILVER v2 + MASK v2 (STILL MASKED)\n",
    "# ================================================\n",
    "\n",
    "def predict_train_probs(model, X_csr, label_feats, A_hat, batch_size=64):\n",
    "    model.eval()\n",
    "    N = X_csr.shape[0]\n",
    "    all_probs = []\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, N, batch_size), desc=\"Predicting Train (Round 1)\"):\n",
    "            X_batch = X_csr[i : i+batch_size].toarray().astype(\"float32\")\n",
    "            X_batch = torch.from_numpy(X_batch).to(device)\n",
    "            logits, _, _ = model(X_batch, label_feats, A_hat)\n",
    "            probs = torch.sigmoid(logits).cpu().numpy()\n",
    "            all_probs.append(probs)\n",
    "    return np.vstack(all_probs)\n",
    "\n",
    "print(\"Predicting train probabilities (Round 1)...\")\n",
    "probs_train = predict_train_probs(\n",
    "    model,\n",
    "    X_train_docs,\n",
    "    label_feats,\n",
    "    A_hat,\n",
    "    batch_size=64\n",
    ")\n",
    "print(\"probs_train shape:\", probs_train.shape)\n",
    "\n",
    "def build_silver_labels_v2_with_mask(\n",
    "    probs,\n",
    "    silver_v1,\n",
    "    mask_v1,\n",
    "    high=0.75,\n",
    "    low=0.25\n",
    "):\n",
    "    \"\"\"\n",
    "    probs  : (N, C)\n",
    "    silver_v1 : (N, C)\n",
    "    mask_v1   : (N, C)\n",
    "    return: silver_v2, mask_v2\n",
    "    - high 이상: 1, mask=1\n",
    "    - low  이하: 0, mask=1\n",
    "    - 그 사이: 이전 silver_v1 / mask_v1 유지\n",
    "    \"\"\"\n",
    "    silver_v2 = silver_v1.copy()\n",
    "    mask_v2   = mask_v1.copy()\n",
    "\n",
    "    high_mask = (probs >= high)\n",
    "    low_mask  = (probs <= low)\n",
    "\n",
    "    silver_v2[high_mask] = 1.0\n",
    "    mask_v2[high_mask]   = 1.0\n",
    "\n",
    "    silver_v2[low_mask] = 0.0\n",
    "    mask_v2[low_mask]   = 1.0\n",
    "\n",
    "    return silver_v2, mask_v2\n",
    "\n",
    "silver_labels_v2, silver_mask_v2 = build_silver_labels_v2_with_mask(\n",
    "    probs_train,\n",
    "    silver_labels_v1,\n",
    "    silver_mask_v1,\n",
    "    high=0.6,   # 살짝 완화\n",
    "    low=0.2\n",
    ")\n",
    "\n",
    "pos_per_doc_v2 = silver_labels_v2.sum(axis=1)\n",
    "sup_per_doc_v2 = silver_mask_v2.sum(axis=1)\n",
    "print(\"[v2] avg positives per doc:\", pos_per_doc_v2.mean())\n",
    "print(\"[v2] avg supervised classes per doc:\", sup_per_doc_v2.mean())\n",
    "print(\"[v2] docs with 0 supervised classes:\", (sup_per_doc_v2 == 0).sum())\n",
    "print(\"Self-training Round 2 labels ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1affd7b7-38b8-4869-ab98-a45617746b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# 10. ROUND 2 TRAINING (MASKED BCE WITH SILVER v2)\n",
    "# ================================================\n",
    "train_dataset_v2 = SilverDatasetWithMask(X_train_docs, silver_labels_v2, silver_mask_v2, pid_list_train)\n",
    "\n",
    "batch_size_round2 = 64\n",
    "train_loader_v2 = DataLoader(\n",
    "    train_dataset_v2,\n",
    "    batch_size=batch_size_round2,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "criterion_round2 = nn.BCEWithLogitsLoss(reduction='none')\n",
    "optimizer_round2 = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs_round2 = 2\n",
    "\n",
    "for epoch in range(1, num_epochs_round2 + 1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_count = 0.0\n",
    "\n",
    "    for batch_x, batch_y, batch_m in tqdm(train_loader_v2, desc=f\"[Round 2] Epoch {epoch}\"):\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "        batch_m = batch_m.to(device)\n",
    "\n",
    "        optimizer_round2.zero_grad()\n",
    "        logits, doc_emb, label_emb = model(batch_x, label_feats, A_hat)\n",
    "        loss_raw = criterion_round2(logits, batch_y)\n",
    "\n",
    "        loss_masked = loss_raw * batch_m\n",
    "        loss = loss_masked.sum() / (batch_m.sum() + 1e-8)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer_round2.step()\n",
    "\n",
    "        total_loss += loss.item() * batch_m.sum().item()\n",
    "        total_count += batch_m.sum().item()\n",
    "\n",
    "    avg_loss = total_loss / (total_count + 1e-8)\n",
    "    print(f\"[Round 2] Epoch {epoch}] avg_loss = {avg_loss:.6f}\")\n",
    "\n",
    "print(\"Round 2 training (masked BCE) finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863c18e6-d0d7-4f04-a398-ccbdcd315433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# 11. FINAL TEST PREDICTION + SUBMISSION\n",
    "#     (model + TF-IDF sims blending, 1~3 labels)\n",
    "# ================================================\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "\n",
    "SUBMISSION_PATH = \"submission.csv\"  # output file\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 1) Round 2 모델 기반 test 확률이 없다면 먼저 예측\n",
    "#    (이미 probs_test가 있다면 이 블록은 건너뛰어도 됨)\n",
    "# ------------------------------------------------\n",
    "def predict_test_probs(model, X_csr, label_feats, A_hat, batch_size=64):\n",
    "    model.eval()\n",
    "    N = X_csr.shape[0]\n",
    "    all_probs = []\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, N, batch_size), desc=\"Predicting test probs\"):\n",
    "            X_batch = X_csr[i : i+batch_size].toarray().astype(\"float32\")\n",
    "            X_batch = torch.from_numpy(X_batch).to(device)\n",
    "            logits, _, _ = model(X_batch, label_feats, A_hat)\n",
    "            probs = torch.sigmoid(logits).cpu().numpy()\n",
    "            all_probs.append(probs)\n",
    "    return np.vstack(all_probs)\n",
    "\n",
    "if \"probs_test\" not in globals():\n",
    "    print(\"probs_test not found in globals, computing...\")\n",
    "    probs_test = predict_test_probs(\n",
    "        model,\n",
    "        X_test_docs,\n",
    "        label_feats,\n",
    "        A_hat,\n",
    "        batch_size=64\n",
    "    )\n",
    "\n",
    "print(\"probs_test shape:\", probs_test.shape)\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 2) TF-IDF 기반 sims_test 계산 (doc–class similarity)\n",
    "# ------------------------------------------------\n",
    "print(\"Computing TF-IDF-based sims for test set...\")\n",
    "\n",
    "# test 문서 정규화\n",
    "X_test_norm = normalize(X_test_docs, axis=1)\n",
    "\n",
    "# 클래스 이름 / 키워드 정규화 행렬은 4번 셀에서 이미 있음:\n",
    "# X_class_name_norm, X_class_kw_norm\n",
    "sims_name_test = (X_test_norm @ X_class_name_norm.T).toarray().astype(\"float32\")\n",
    "sims_kw_test   = (X_test_norm @ X_class_kw_norm.T).toarray().astype(\"float32\")\n",
    "\n",
    "# 이름 vs 키워드 비중\n",
    "alpha = 0.3  # class name weight\n",
    "beta  = 0.7  # keyword weight\n",
    "\n",
    "sims_test = alpha * sims_name_test + beta * sims_kw_test\n",
    "print(\"sims_test shape:\", sims_test.shape)\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 3) MODEL + SIMS BLENDING\n",
    "# ------------------------------------------------\n",
    "lambda_model = 0.6\n",
    "lambda_sims  = 0.4\n",
    "\n",
    "final_scores = lambda_model * probs_test + lambda_sims * sims_test\n",
    "print(\"final_scores shape:\", final_scores.shape)\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 4) 동적 라벨 선택 함수\n",
    "#    - threshold 이상인 클래스만 후보\n",
    "#    - 없으면 top-1\n",
    "#    - 너무 많으면 상위 MAX_LABELS개\n",
    "#    - MIN_LABELS, MAX_LABELS는 baseline에서 정의한 값 사용\n",
    "# ------------------------------------------------\n",
    "def pick_labels(score_row, min_k=1, max_k=3, threshold=0.6):\n",
    "    \"\"\"\n",
    "    score_row: shape (C,) - 한 문서에 대한 클래스 점수 벡터\n",
    "    min_k    : 최소 라벨 개수 (보통 1)\n",
    "    max_k    : 최대 라벨 개수 (보통 3)\n",
    "    threshold: 이 값 이상인 클래스만 '유망하다'고 보고 우선 선택\n",
    "    \"\"\"\n",
    "    # 1) threshold 이상인 클래스만 후보로 뽑기\n",
    "    cand = np.where(score_row >= threshold)[0]\n",
    "\n",
    "    # 2) threshold 넘는 게 하나도 없으면 → top-1만 선택\n",
    "    if len(cand) == 0:\n",
    "        top1 = int(np.argmax(score_row))\n",
    "        return [top1]\n",
    "\n",
    "    # 3) 후보가 너무 많으면 → 점수 순으로 max_k개만 남기기\n",
    "    if len(cand) > max_k:\n",
    "        sorted_idx = cand[np.argsort(score_row[cand])[::-1]]\n",
    "        cand = sorted_idx[:max_k]\n",
    "\n",
    "    # 4) 후보가 min_k보다 적으면 (예: cand 1개인데 min_k=2) → top-score에서 채워서 min_k까지 맞추기\n",
    "    if len(cand) < min_k:\n",
    "        sorted_idx = np.argsort(score_row)[::-1]\n",
    "        for c in sorted_idx:\n",
    "            if c not in cand:\n",
    "                cand = np.append(cand, c)\n",
    "                if len(cand) == min_k:\n",
    "                    break\n",
    "\n",
    "    return list(map(int, cand))\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 5) baseline 스타일로 submission.csv 생성\n",
    "# ------------------------------------------------\n",
    "all_pids, all_labels = [], []\n",
    "\n",
    "print(\"Generating final predictions for submission...\")\n",
    "for i, pid in enumerate(tqdm(pid_list_test, desc=\"Scoring test instances\")):\n",
    "    scores = final_scores[i]\n",
    "    labels = pick_labels(\n",
    "        scores,\n",
    "        min_k=MIN_LABELS,\n",
    "        max_k=MAX_LABELS,\n",
    "        threshold=0.6,  # 필요하면 조정 가능\n",
    "    )\n",
    "    labels = sorted(labels)\n",
    "    all_pids.append(pid)\n",
    "    all_labels.append(labels)\n",
    "\n",
    "# --- Save submission file (baseline 포맷 그대로) ---\n",
    "with open(SUBMISSION_PATH, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"id\", \"labels\"])\n",
    "    for pid, labels in zip(all_pids, all_labels):\n",
    "        writer.writerow([pid, \",\".join(map(str, labels))])\n",
    "\n",
    "print(f\"Submission file saved to: {SUBMISSION_PATH}\")\n",
    "print(f\"Total samples: {len(all_pids)}, Classes per sample: {MIN_LABELS}-{MAX_LABELS} (dynamic)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54be3c54-d2b3-43b0-be01-1b7cf2e736ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = [len(lbls) for lbls in all_labels]\n",
    "print(\"min labels per doc:\", min(lengths))\n",
    "print(\"max labels per doc:\", max(lengths))\n",
    "\n",
    "unique, counts = np.unique(lengths, return_counts=True)\n",
    "print(\"label count distribution:\")\n",
    "for u, c in zip(unique, counts):\n",
    "    print(f\"{u} labels: {c} docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ce8f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 서로 다른 두 개 train 샘플에 대해 probs 비교\n",
    "print(probs_test[0][:10])\n",
    "print(probs_test[1][:10])\n",
    "\n",
    "# 혹은 표준편차\n",
    "print(\"std over classes (sample 0):\", probs_test[0].std())\n",
    "print(\"std over classes (sample 1):\", probs_test[1].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063eab09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# silver v1 통계\n",
    "pos_per_doc_v1 = silver_labels_v1.sum(axis=1)\n",
    "print(\"[v1] avg positives per doc:\", pos_per_doc_v1.mean())\n",
    "print(\"[v1] min positives per doc:\", pos_per_doc_v1.min())\n",
    "print(\"[v1] docs with 0 positives:\", (pos_per_doc_v1 == 0).sum())\n",
    "\n",
    "# silver v2 통계\n",
    "pos_per_doc_v2 = silver_labels_v2.sum(axis=1)\n",
    "print(\"[v2] avg positives per doc:\", pos_per_doc_v2.mean())\n",
    "print(\"[v2] min positives per doc:\", pos_per_doc_v2.min())\n",
    "print(\"[v2] docs with 0 positives:\", (pos_per_doc_v2 == 0).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c062713b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886663cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084431fe-97fd-442e-9bb7-78f1153f80c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b486e5-1fb7-4e9e-b63a-bc3919ab81ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e97ef5e-5ee1-4bb1-9ccd-d0242241c2b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312fa965-ac9f-4255-a3d7-3466aba485f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7419c2a8-6739-4ed6-b9ee-3eeb108873de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T09:43:38.269438Z",
     "iopub.status.busy": "2025-11-16T09:43:38.269177Z",
     "iopub.status.idle": "2025-11-16T09:43:38.378857Z",
     "shell.execute_reply": "2025-11-16T09:43:38.377909Z",
     "shell.execute_reply.started": "2025-11-16T09:43:38.269420Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating dummy predictions: 100%|██████████| 19658/19658 [00:00<00:00, 312260.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy submission file saved to: submission.csv\n",
      "Total samples: 19658, Classes per sample: 1-3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ------------------------\n",
    "# Dummy baseline for Kaggle submission\n",
    "# Generates random multi-label predictions\n",
    "# ------------------------\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Paths ---\n",
    "TEST_DIR = \"Amazon_products/test\"  # modify if needed\n",
    "TEST_CORPUS_PATH = os.path.join(TEST_DIR, \"test_corpus.txt\")  # product_id \\t text\n",
    "SUBMISSION_PATH = \"submission.csv\"  # output file\n",
    "\n",
    "# --- Constants ---\n",
    "NUM_CLASSES = 531  # total number of classes (0–530)\n",
    "MIN_LABELS = 1     # minimum number of labels per sample\n",
    "MAX_LABELS = 3     # maximum number of labels per sample\n",
    "\n",
    "# --- Load test corpus ---\n",
    "def load_corpus(path):\n",
    "    \"\"\"Load test corpus into {pid: text} dictionary.\"\"\"\n",
    "    pid2text = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\"\\t\", 1)\n",
    "            if len(parts) == 2:\n",
    "                pid, text = parts\n",
    "                pid2text[pid] = text\n",
    "    return pid2text\n",
    "\n",
    "pid2text_test = load_corpus(TEST_CORPUS_PATH)\n",
    "pid_list_test = list(pid2text_test.keys())\n",
    "\n",
    "# --- Generate random predictions ---\n",
    "all_pids, all_labels = [], []\n",
    "for pid in tqdm(pid_list_test, desc=\"Generating dummy predictions\"):\n",
    "    n_labels = random.randint(MIN_LABELS, MAX_LABELS)\n",
    "    labels = random.sample(range(NUM_CLASSES), n_labels)\n",
    "    labels = sorted(labels)\n",
    "    all_pids.append(pid)\n",
    "    all_labels.append(labels)\n",
    "\n",
    "# --- Save submission file ---\n",
    "with open(SUBMISSION_PATH, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"pid\", \"labels\"])\n",
    "    for pid, labels in zip(all_pids, all_labels):\n",
    "        writer.writerow([pid, \",\".join(map(str, labels))])\n",
    "\n",
    "print(f\"Dummy submission file saved to: {SUBMISSION_PATH}\")\n",
    "print(f\"Total samples: {len(all_pids)}, Classes per sample: {MIN_LABELS}-{MAX_LABELS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c888409a-151d-4131-8d5b-b885b848eded",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ESCI Environment (Python 3.9)",
   "language": "python",
   "name": "esci"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
