{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bd9de5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T11:08:16.147036Z",
     "iopub.status.busy": "2025-12-01T11:08:16.146726Z",
     "iopub.status.idle": "2025-12-01T11:08:17.649205Z",
     "shell.execute_reply": "2025-12-01T11:08:17.648174Z",
     "shell.execute_reply.started": "2025-12-01T11:08:16.147014Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "\n",
      "== Data path check ==\n",
      "Amazon_products/train/train_corpus.txt -> True\n",
      "Amazon_products/test/test_corpus.txt -> True\n",
      "Amazon_products/classes.txt -> True\n",
      "Amazon_products/class_hierarchy.txt -> True\n",
      "Amazon_products/class_related_keywords.txt -> True\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# 1. REPRODUCIBILITY SETTINGS \n",
    "# ================================================\n",
    "from pathlib import Path\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "# ----- Reproducibility -----\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# ----- Device -----\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# ================================================\n",
    "# 2. DEFAULT PATHS FOR DATASET\n",
    "# ================================================\n",
    "ROOT = Path(\"Amazon_products\")   # dataset root directory\n",
    "\n",
    "# Main corpus\n",
    "TRAIN_CORPUS_PATH = ROOT / \"train\" /  \"train_corpus.txt\"       # pid \\t text\n",
    "TEST_CORPUS_PATH  = ROOT / \"test\" / \"test_corpus.txt\"        # pid \\t text\n",
    "\n",
    "# Taxonomy & class meta\n",
    "CLASSES_PATH      = ROOT / \"classes.txt\"            # class_id \\t class_name\n",
    "HIERARCHY_PATH    = ROOT / \"class_hierarchy.txt\"    # parent_id \\t child_id\n",
    "KEYWORDS_PATH     = ROOT / \"class_related_keywords.txt\"\n",
    "\n",
    "# Constants\n",
    "NUM_CLASSES = 531\n",
    "MIN_LABELS = 2     # minimum number of labels per sample\n",
    "MAX_LABELS = 3     # maximum number of labels per sample\n",
    "\n",
    "# Check paths\n",
    "print(\"\\n== Data path check ==\")\n",
    "for p in [TRAIN_CORPUS_PATH, TEST_CORPUS_PATH,\n",
    "          CLASSES_PATH, HIERARCHY_PATH, KEYWORDS_PATH]:\n",
    "    print(f\"{p} -> {p.exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9f8281b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T11:08:17.650226Z",
     "iopub.status.busy": "2025-12-01T11:08:17.649987Z",
     "iopub.status.idle": "2025-12-01T11:08:17.790700Z",
     "shell.execute_reply": "2025-12-01T11:08:17.789921Z",
     "shell.execute_reply.started": "2025-12-01T11:08:17.650209Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train/test corpus...\n",
      "Train samples: 29487\n",
      "Test samples : 19658\n",
      "Example train sample #0: pid=0, text=omron hem 790it automatic blood pressure monitor with advanced omron health mana...\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# 2. DATA LOADING\n",
    "# ================================================\n",
    "\n",
    "def load_corpus(path):\n",
    "    \"\"\"\n",
    "    Load corpus file (pid \\\\t text) as {pid: text} dictionary.\n",
    "    \"\"\"\n",
    "    pid2text = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\"\\t\", 1)\n",
    "            if len(parts) == 2:\n",
    "                pid, text = parts\n",
    "                pid2text[pid] = text\n",
    "    return pid2text\n",
    "\n",
    "# Load corpus\n",
    "print(\"Loading train/test corpus...\")\n",
    "\n",
    "pid2text_train = load_corpus(TRAIN_CORPUS_PATH)\n",
    "pid2text_test  = load_corpus(TEST_CORPUS_PATH)\n",
    "\n",
    "pid_list_train = list(pid2text_train.keys())\n",
    "pid_list_test  = list(pid2text_test.keys())\n",
    "\n",
    "print(\"Train samples:\", len(pid2text_train))\n",
    "print(\"Test samples :\", len(pid2text_test))\n",
    "\n",
    "# Quick sample check\n",
    "for i, (pid, text) in enumerate(pid2text_train.items()):\n",
    "    print(f\"Example train sample #{i}: pid={pid}, text={text[:80]}...\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acad81ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T11:08:21.282410Z",
     "iopub.status.busy": "2025-12-01T11:08:21.282140Z",
     "iopub.status.idle": "2025-12-01T11:08:21.296129Z",
     "shell.execute_reply": "2025-12-01T11:08:21.295601Z",
     "shell.execute_reply.started": "2025-12-01T11:08:21.282389Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading class metadata...\n",
      "Num classes: 531\n",
      "Num edges in taxonomy: 568\n",
      "\n",
      "Example class id: 0\n",
      "Name: grocery_gourmet_food\n",
      "Keywords: ['snacks', 'condiments', 'beverages', 'specialty_foods', 'spices', 'cooking_oils', 'baking_ingredients', 'gourmet_chocolates', 'artisanal_cheeses', 'organic_foods']\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# 3. CLASS METADATA LOADING\n",
    "# ================================================\n",
    "\n",
    "def load_classes(path):\n",
    "    \"\"\"\n",
    "    classes.txt : class_id \\\\t class_name\n",
    "    returns: id2label, label2id\n",
    "    \"\"\"\n",
    "    id2label = {}\n",
    "    label2id = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\"\\t\")\n",
    "            if len(parts) != 2:\n",
    "                continue\n",
    "            cid, name = parts\n",
    "            cid = int(cid)\n",
    "            id2label[cid] = name\n",
    "            label2id[name] = cid\n",
    "    return id2label, label2id\n",
    "\n",
    "\n",
    "def load_hierarchy(path):\n",
    "    \"\"\"\n",
    "    class_hierarchy.txt : parent_id \\\\t child_id\n",
    "    returns: edges (list of tuples)\n",
    "    \"\"\"\n",
    "    edges = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\"\\t\")\n",
    "            if len(parts) != 2:\n",
    "                continue\n",
    "            p, c = map(int, parts)\n",
    "            edges.append((p, c))\n",
    "    return edges\n",
    "\n",
    "\n",
    "def load_keywords(path, label2id):\n",
    "    \"\"\"\n",
    "    class_related_keywords.txt : CLASS_NAME: kw1, kw2,...\n",
    "    returns: {class_id: [kws]}\n",
    "    \"\"\"\n",
    "    d = {cid: [] for cid in label2id.values()}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if \":\" not in line:\n",
    "                continue\n",
    "            name, kws = line.strip().split(\":\", 1)\n",
    "            kws = [k.strip() for k in kws.split(\",\") if k.strip()]\n",
    "            if name in label2id:\n",
    "                cid = label2id[name]\n",
    "                d[cid] = kws\n",
    "    return d\n",
    "\n",
    "\n",
    "# ----------------- Load all class meta -----------------\n",
    "print(\"Loading class metadata...\")\n",
    "\n",
    "id2label, label2id = load_classes(CLASSES_PATH)\n",
    "edges = load_hierarchy(HIERARCHY_PATH)\n",
    "label_keywords = load_keywords(KEYWORDS_PATH, label2id)\n",
    "\n",
    "print(\"Num classes:\", len(id2label))\n",
    "print(\"Num edges in taxonomy:\", len(edges))\n",
    "print()\n",
    "\n",
    "# Small check\n",
    "example_id = 0\n",
    "print(\"Example class id:\", example_id)\n",
    "print(\"Name:\", id2label[example_id])\n",
    "print(\"Keywords:\", label_keywords[example_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23f5baa9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T11:11:09.976634Z",
     "iopub.status.busy": "2025-12-01T11:11:09.976348Z",
     "iopub.status.idle": "2025-12-01T11:14:59.407236Z",
     "shell.execute_reply": "2025-12-01T11:14:59.406525Z",
     "shell.execute_reply.started": "2025-12-01T11:11:09.976613Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== Loading embedding model: Alibaba-NLP/gte-base-en-v1.5 ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/Alibaba-NLP/new-impl:\n",
      "- configuration.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/Alibaba-NLP/new-impl:\n",
      "- modeling.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dir: Amazon_products/embeddings\n",
      "\n",
      "== Building document embeddings (train/test) ==\n",
      "Num train texts: 29487\n",
      "Num test texts : 19658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 461/461 [02:10<00:00,  3.52it/s]\n",
      "Batches: 100%|██████████| 308/308 [01:30<00:00,  3.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_doc_emb shape: (29487, 768)\n",
      "test_doc_emb shape : (19658, 768)\n",
      "\n",
      "== Building class embeddings ==\n",
      "Num class ids (from id2label): 531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 9/9 [00:00<00:00, 56.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_name_emb shape: (531, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 9/9 [00:01<00:00,  7.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_kw_emb shape: (531, 768)\n",
      "\n",
      "== GTE embedding step completed successfully. ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# 4. TEXT EMBEDDING WITH GTE (Alibaba-NLP/gte-base-en-v1.5)\n",
    "# ================================================\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# ----- Embedding model -----\n",
    "print(f\"\\n== Loading embedding model: {EMB_MODEL_NAME} ==\")\n",
    "emb_model = SentenceTransformer(\n",
    "    \"Alibaba-NLP/gte-base-en-v1.5\",\n",
    "    device=device,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "\n",
    "def encode_texts(texts, batch_size: int = 64, normalize: bool = True):\n",
    "    \"\"\"\n",
    "    Alibaba-NLP/gte-base-en-v1.5로 텍스트 리스트 임베딩.\n",
    "    - 파라미터는 고정 (feature extractor로만 사용)\n",
    "    - normalize=True 이면 cosine similarity 계산이 쉬워짐\n",
    "    \"\"\"\n",
    "    embeddings = emb_model.encode(\n",
    "        texts,\n",
    "        batch_size=batch_size,\n",
    "        convert_to_numpy=True,\n",
    "        show_progress_bar=True,\n",
    "        normalize_embeddings=normalize,\n",
    "    )\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "# ----- Save directory for embeddings -----\n",
    "EMB_DIR = ROOT / \"embeddings\"\n",
    "EMB_DIR.mkdir(exist_ok=True, parents=True)\n",
    "print(\"Embedding dir:\", EMB_DIR)\n",
    "\n",
    "\n",
    "# ================================================\n",
    "# 4-1. DOCUMENT EMBEDDINGS (TRAIN / TEST)\n",
    "# ================================================\n",
    "print(\"\\n== Building document embeddings (train/test) ==\")\n",
    "\n",
    "# pid 순서에 맞게 텍스트 리스트 만들기\n",
    "train_texts = [pid2text_train[pid] for pid in pid_list_train]\n",
    "test_texts  = [pid2text_test[pid] for pid in pid_list_test]\n",
    "\n",
    "print(\"Num train texts:\", len(train_texts))\n",
    "print(\"Num test texts :\", len(test_texts))\n",
    "\n",
    "# 임베딩 계산\n",
    "train_doc_emb = encode_texts(train_texts)\n",
    "test_doc_emb  = encode_texts(test_texts)\n",
    "\n",
    "print(\"train_doc_emb shape:\", train_doc_emb.shape)\n",
    "print(\"test_doc_emb shape :\", test_doc_emb.shape)\n",
    "\n",
    "# 저장\n",
    "np.save(EMB_DIR / \"train_doc_gte.npy\", train_doc_emb)\n",
    "np.save(EMB_DIR / \"test_doc_gte.npy\",  test_doc_emb)\n",
    "\n",
    "# pid 리스트도 같이 저장 (submission 만들 때 필요)\n",
    "np.save(EMB_DIR / \"pid_list_train.npy\", np.array(pid_list_train))\n",
    "np.save(EMB_DIR / \"pid_list_test.npy\",  np.array(pid_list_test))\n",
    "\n",
    "\n",
    "# ================================================\n",
    "# 4-2. CLASS EMBEDDINGS (NAME / NAME+KEYWORDS)\n",
    "# ================================================\n",
    "print(\"\\n== Building class embeddings ==\")\n",
    "\n",
    "# 클래스 id를 정렬된 순서로 사용 (0 ~ NUM_CLASSES-1라고 가정)\n",
    "class_ids = sorted(id2label.keys())\n",
    "print(\"Num class ids (from id2label):\", len(class_ids))\n",
    "\n",
    "# 혹시라도 NUM_CLASSES와 다르면 바로 잡기\n",
    "if len(class_ids) != NUM_CLASSES:\n",
    "    print(f\"[WARN] NUM_CLASSES({NUM_CLASSES}) != loaded classes({len(class_ids)})\")\n",
    "\n",
    "# (1) class_name 임베딩\n",
    "class_names = [id2label[cid] for cid in class_ids]\n",
    "class_name_emb = encode_texts(class_names)\n",
    "print(\"class_name_emb shape:\", class_name_emb.shape)\n",
    "\n",
    "np.save(EMB_DIR / \"class_name_gte.npy\", class_name_emb)\n",
    "\n",
    "# (2) class_name + keywords 합친 텍스트로 임베딩 (선택, 나중에 실험할 때 사용 가능)\n",
    "merged_class_texts = []\n",
    "for cid in class_ids:\n",
    "    name = id2label[cid]\n",
    "    kws = label_keywords.get(cid, [])\n",
    "    if kws:\n",
    "        merged = name + \" : \" + \", \".join(kws)\n",
    "    else:\n",
    "        merged = name\n",
    "    merged_class_texts.append(merged)\n",
    "\n",
    "class_kw_emb = encode_texts(merged_class_texts)\n",
    "print(\"class_kw_emb shape:\", class_kw_emb.shape)\n",
    "\n",
    "np.save(EMB_DIR / \"class_kw_gte.npy\", class_kw_emb)\n",
    "\n",
    "print(\"\\n== GTE embedding step completed successfully. ==\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bc52ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# 5. LOAD PRECOMPUTED EMBEDDINGS\n",
    "# ================================================\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 이미 위에서 ROOT, NUM_CLASSES, device 가 정의되어 있다고 가정\n",
    "EMB_DIR = ROOT / \"embeddings\"\n",
    "print(\"Embedding dir:\", EMB_DIR)\n",
    "\n",
    "\n",
    "# ----- 5-1. 기본 임베딩 로딩 -----\n",
    "print(\"\\n== Loading precomputed embeddings ==\")\n",
    "\n",
    "# (1) 문서 임베딩\n",
    "train_doc_emb = np.load(EMB_DIR / \"train_doc_gte.npy\")   # shape: (N_train, dim)\n",
    "test_doc_emb  = np.load(EMB_DIR / \"test_doc_gte.npy\")    # shape: (N_test, dim)\n",
    "\n",
    "# (2) pid 리스트\n",
    "pid_list_train = np.load(EMB_DIR / \"pid_list_train.npy\", allow_pickle=True)\n",
    "pid_list_test  = np.load(EMB_DIR / \"pid_list_test.npy\",  allow_pickle=True)\n",
    "\n",
    "# (3) 클래스 임베딩 (이름 기반 / 이름+키워드 기반 두 종류)\n",
    "class_name_emb = np.load(EMB_DIR / \"class_name_gte.npy\") # shape: (NUM_CLASSES, dim)\n",
    "class_kw_emb   = np.load(EMB_DIR / \"class_kw_gte.npy\")   # shape: (NUM_CLASSES, dim)\n",
    "\n",
    "print(\"train_doc_emb:\", train_doc_emb.shape)\n",
    "print(\"test_doc_emb :\", test_doc_emb.shape)\n",
    "print(\"class_name_emb:\", class_name_emb.shape)\n",
    "print(\"class_kw_emb  :\", class_kw_emb.shape)\n",
    "print(\"len(pid_list_train):\", len(pid_list_train))\n",
    "print(\"len(pid_list_test) :\", len(pid_list_test))\n",
    "\n",
    "\n",
    "# ================================================\n",
    "# 5-2. TORCH TENSOR로 변환 (학습에 바로 쓰기 용)\n",
    "# ================================================\n",
    "train_doc_tensor = torch.from_numpy(train_doc_emb).float().to(device)\n",
    "test_doc_tensor  = torch.from_numpy(test_doc_emb).float().to(device)\n",
    "\n",
    "class_name_tensor = torch.from_numpy(class_name_emb).float().to(device)\n",
    "class_kw_tensor   = torch.from_numpy(class_kw_emb).float().to(device)\n",
    "\n",
    "print(\"\\nTensors moved to device:\", device)\n",
    "print(\"train_doc_tensor:\", train_doc_tensor.shape)\n",
    "print(\"class_name_tensor:\", class_name_tensor.shape)\n",
    "\n",
    "\n",
    "# ================================================\n",
    "# 5-3. 학습용 Dataset/Dataloader 예시 (라벨 생기면 여기에 붙이면 됨)\n",
    "# ================================================\n",
    "\n",
    "class EmbeddingDataset(Dataset):\n",
    "    \"\"\"\n",
    "    X: (N, dim) 문서 임베딩\n",
    "    y: (N, C) multi-hot 라벨 (TaxoClass-style silver label 들어갈 자리)\n",
    "    \"\"\"\n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray):\n",
    "        assert X.shape[0] == y.shape[0]\n",
    "        self.X = torch.from_numpy(X).float()\n",
    "        self.y = torch.from_numpy(y).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f5a005c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T11:36:05.335424Z",
     "iopub.status.busy": "2025-12-01T11:36:05.335123Z",
     "iopub.status.idle": "2025-12-01T11:36:05.931725Z",
     "shell.execute_reply": "2025-12-01T11:36:05.931195Z",
     "shell.execute_reply.started": "2025-12-01T11:36:05.335400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_doc_emb: (19658, 768)\n",
      "class_emb   : (531, 768)\n",
      "sim_matrix: (19658, 531)\n",
      "예시 labels 5개: ['473,197,461', '168,17,18', '300,73,112', '304,354,0', '220,508,109']\n",
      "예시 pid 5개   : ['0' '1' '2' '3' '4']\n",
      "Saved submission to: Amazon_products/submissions/submission_top3_cosine.csv\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# 5. COSINE SIMILARITY → TOP-3 LABEL → CSV SUBMISSION\n",
    "# ================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ----- 5-1. 임베딩 로드 -----\n",
    "EMB_DIR = ROOT / \"embeddings\"\n",
    "\n",
    "# (1) 텍스트 임베딩: test 문서\n",
    "test_doc_emb = np.load(EMB_DIR / \"test_doc_gte.npy\")      # shape: (N_test, dim)\n",
    "pid_list_test = np.load(EMB_DIR / \"pid_list_test.npy\", allow_pickle=True)  # shape: (N_test,)\n",
    "\n",
    "# (2) 클래스 임베딩: 여기서는 class_name 기반 사용 (원하면 class_kw_gte로 바꿔도 됨)\n",
    "class_emb = np.load(EMB_DIR / \"class_name_gte.npy\")       # shape: (NUM_CLASSES, dim)\n",
    "\n",
    "print(\"test_doc_emb:\", test_doc_emb.shape)\n",
    "print(\"class_emb   :\", class_emb.shape)\n",
    "\n",
    "\n",
    "# ----- 5-2. 코사인 유사도 계산 (dot product: 이미 normalize_embeddings=True) -----\n",
    "# sim[i, j] = cos(test_doc_i, class_j)\n",
    "sim_matrix = np.matmul(test_doc_emb, class_emb.T)   # shape: (N_test, NUM_CLASSES)\n",
    "print(\"sim_matrix:\", sim_matrix.shape)\n",
    "\n",
    "\n",
    "# ----- 5-3. 각 문서마다 top-3 클래스 뽑기 -----\n",
    "TOP_K = 3\n",
    "\n",
    "# sim_matrix: (N_test, NUM_CLASSES)\n",
    "# argsort를 이용해 각 행에서 상위 TOP_K 인덱스를 구함\n",
    "# np.argpartition을 써도 되지만, NUM_CLASSES=531이라 그냥 argsort로 가도 충분히 빠름.\n",
    "topk_indices = np.argsort(-sim_matrix, axis=1)[:, :TOP_K]   # shape: (N_test, TOP_K)\n",
    "\n",
    "# labels는 공백으로 이어붙인 \"cid1 cid2 cid3\" 형태로 만듦\n",
    "labels_str_list = []\n",
    "for row in topk_indices:\n",
    "    # 정수형 class id 리스트 → 문자열 리스트 → 공백으로 join\n",
    "    cids = [str(int(cid)) for cid in row]\n",
    "    labels_str = \",\".join(cids)\n",
    "    labels_str_list.append(labels_str)\n",
    "\n",
    "print(\"예시 labels 5개:\", labels_str_list[:5])\n",
    "print(\"예시 pid 5개   :\", pid_list_test[:5])\n",
    "\n",
    "\n",
    "# ----- 5-4. Kaggle 제출용 CSV 만들기 -----\n",
    "# Kaggle 포맷 가정: 컬럼 이름이 'id', 'labels'\n",
    "# (previous warning 메시지에도 'id column' 언급이 있었으니 'id'로 맞추는 게 안전)\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": pid_list_test,\n",
    "    \"labels\": labels_str_list\n",
    "})\n",
    "\n",
    "SUBMISSION_PATH = ROOT / \"submissions\" / \"submission_top3_cosine.csv\"\n",
    "SUBMISSION_PATH.parent.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "submission.to_csv(SUBMISSION_PATH, index=False)\n",
    "print(\"Saved submission to:\", SUBMISSION_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbc95d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acef0cd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad7505f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f952c95c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7199d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69983555",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ESCI Environment (Python 3.9)",
   "language": "python",
   "name": "esci"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
